\documentclass[twocolumn,5p]{elsarticle}
\usepackage{tikz}
\def\firstcircle{(90:1.75cm) circle (2.5cm)}
\def\secondcircle{(210:1.75cm) circle (2.5cm)}
\def\thirdcircle{(330:1.75cm) circle (2.5cm)} 
\usepackage{comment}
\usepackage{framed,graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{bigstrut}
\usepackage{color}
\usepackage{graphics} 
\usepackage{eqparbox}
\usepackage{graphics}
\usepackage{colortbl} 
%\usepackage{times}
\usepackage{mathptmx} \usepackage[scaled=.90]{helvet} \usepackage{courier}
\usepackage{balance}
\usepackage{picture}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
\renewcommand{\footnotesize}{\scriptsize}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{lavenderpink}{rgb}{0.98, 0.68, 0.82}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\quart}[4]{\begin{picture}(80,4)%1
	{\color{black}\put(#3,2){\circle*{4}}\put(#1,2){\line(1,0){#2}}}\end{picture}}

\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}

\newcommand{\wei}[1]{\textcolor{red}{Wei: #1}} 
\newcommand{\Menzies}[1]{\textcolor{red}{Dr.Menzies: #1}} 

%% timm tricks
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
	\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}


\usepackage[shortlabels]{enumitem}  
\usepackage{url}

\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
% \usepackage[svgnames]{xcolor}
\usepackage[framed]{ntheorem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shadows}
%\newtheorem{Lesson}{Lesson}
\theoremclass{Lesson}
\theoremstyle{break}

% inner sep=10pt,
\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black,
fill=Gray!40,  drop shadow={fill=black, opacity=1}]
\newcommand\thmbox[1]{%
	\noindent\begin{tikzpicture}%
	\node [thmbox] (box){%
		\begin{minipage}{.94\textwidth}%    
		\vspace{-2mm}#1\vspace{-3mm}%
		\end{minipage}%
	};%
	\end{tikzpicture}}

\let\theoremframecommand\thmbox
\newshadedtheorem{lesson}{Result}



\begin{document}
	
	\begin{frontmatter}
		
		\title{Recommendations for Intelligent Code Reorganization}
		
		\author{Rahul Krishna\corref{cor1}\textsuperscript{a,}}
		\ead{rkrish11@ncsu.edu}
		\author{Tim Menzies\corref{cor1}\textsuperscript{a,}}
		\ead{tim.menzies@gmail.com}
		\author{Lucas Layman\textsuperscript{b}}
		\ead{ llayman@cese.fraunhofer.org }
		\cortext[cor1]{Corresponding author: Tel:+1-919-396-4143(Rahul)}
		\address{\textsuperscript{a}Department of Computer Science, North Carolina State University, Raleigh, NC, USA\\
			\textsuperscript{b}Fraunhofer CESE, College Park, USA}
		\pagenumbering{arabic}
		
		\begin{abstract} 
			{\bf Context: } 
			Developers use bad code smells to guide code reorganization.
			Yet developers, text books, tools, and researchers disagree on which bad smells are important.
			%Many bad smell detectors recommend refactoring when static code measure
			%cross some threshold boundary. 
			%%However, it is unclear at what threshold values smell-related metrics
			%should trigger code improvement or preventative maintenance.  
			
			\noindent 
			{\bf Objective:} To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code.
			
			\noindent
			{\bf Method: } We introduce XTREE, a tool that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes.
			Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems).
			
			\noindent
			{\bf Evaluation: } We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts.  
			
			\noindent
			{\bf Results: }Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations in previous studies. Further, XTREE endorses changes to very few code metrics, and those bad smell recommendations (learned from previous studies) are not universal to all  software projects.
			
			\noindent
			{\bf Conclusion: }
			Before undertaking a code reorganization based on a bad smell report,
			use a tool like XTREE to check and ignore any such operations that  are useless; i.e. which lacks evidence
			in the historical record  that it is useful to make that change.
			Note that this use case applies to both manual code reorganizations proposed by developers
			as well as those conducted by automatic methods.
			This recommendation assumes that there is an historical record.
			If none exists, then the results of this paper could be use used as a guide (see our
			Table~8). 
			
			
		\end{abstract}
	\end{frontmatter}
	\pagenumbering{arabic} %XXX delete before submission
	
	\vspace{1mm}
	\noindent
	{\bf Keywords:} Bad smells,
	performance prediction,  decision trees 
	
	
	
	
	\section{Introduction}
	
	According to   Fowler~\cite{fowler99}, bad smells (a.k.a. code smells)
	are ``a surface indication that usually corresponds to a deeper problem''.
	Fowler  recommends   removing   code smells   by
	\begin{quote}
		``...applying a series of small behavior-preserving transformations, each 
		of which seem ``too small to be worth doing''. 
		The  effect of   these refactoring transformations is quite significant. By doing them in small steps you reduce the risk 
		of introducing errors''.
	\end{quote}
	
% 	Bad smells are a widely used technique for evaluating code. 
	While the original concept of bad smells was largely subjective, researchers including Marinescu~\cite{marinescu06} and Munro~\cite{munro2005product} provide definitions of ``bad smells'' in terms of static code attributes such size, complexity, coupling, and other metrics. Consequently, code smells are captured by popular static analysis tools such as PMD\footnote{https://github.com/pmd/pmd}, CheckStyle\footnote{http://checkstyle.sourceforge.net/}, FindBugs\footnote{http://findbugs.sourceforge.net/}, and SonarQube\footnote{http://www.sonarqube.org/}. 

	
	The value of correcting bad smells is not always perceivable. For example, this paper began when a Washington-based software company shared one of their management challenges with us: their  releases are delayed by developers spending  much time removing bad smells within their code. Kim et al.~\cite{kim2012field} surveyed developers at Microsoft and found that code reorganizations incur significant cost and risks. Researchers are actively attempting to demonstrate the actual costs and benefits of fixing bad smells~\cite{nugroho2011empirical,zazworka2011prioritizing,zazworka2013case}, though more case studies are needed.
	
			We refer to the process of removing bad smells as \textit{code reorganization}. Code reorganization is an amalgum of perfective and preventive maintenance~\cite{iso14764}. In contrast to refactoring, code reorganization is not guaranteed to preserve behavior.
			
	Fowler~\cite{fowler99} and other influential software practitioners~\cite{mcconnell2004code,horror} recommend refactoring and code reorganization to remove bad smells. Experiments suggest a relationship between code smells and poor maintainability or defect proneness~\cite{yamashita2013code,yama13,zazworka2011investigating}, though these findings are not always consistent~\cite{olbrich2010all}. In this paper, we focus on the challenge of recommending code reorganizations that result in perceivable benefits (such as reduced defect proneness) and avoid those reorganizations which have no demonstrable benefit and thus waste effort.
	
	
	
	%A Google search on the terms ``refactoring cost'' yields numerous developer questions on StackOverflow asking how to justify bad smell removal and when the costs outweigh the benefits. 
	
	
	% As of  early 2016, the ACM Digital Library reports 
	% the Fowler text of {\em Refactoring} has 6434 references~\cite{fowler99}. A search  on \texttt{``bad smell'' programming 
	% where proceedings=ICSE} in the ACM Digital Library finds 1,748 papers since 2006.
	% Hence it is important to understand the value of
	% this popular topic.  
	% Valuable development resources may be wasted on bad smells that are ``bad''; i.e. 
	%  irrelevant or misleading
	% for some particular project.
	
	% In our experience, programmers can use the SE literature to justify extensive, perhaps even excessive, refactoring efforts.
	% For example, this paper began when a Washington-based software company shared
	% one of their management challenges with us: their  releases are delayed by developers spending  much time removing bad smells within their code. This management team wanted some way to check if all that activity was useful.
	
	% In addition to tools that capture bad smells such as FindBugs and SonarQube, a multitude of tools have been proposed to support refactoring (e.g., \cite{ge2014manual,lee2013drag,foster2012witchdoctor,ge2012reconciling}). Note that these managers are {\em not} asking for better
	% automatic refactoring
	% tool support. 
	% Their human programmers are refactoring
	% code all the time, perhaps too much so.
	% Hence, the management goal
	
	This paper evaluates XTREE~\cite{krishna2015actionable}, a framework to evaluate whether a code reorganization is likely to have a perceivable benefit in terms of defect-proneness. We focus on bad smells indicated by code metrics such as size and complexity as captured in popular tools such as SonarQube and Klockwork\footnote{http://www.klocwork.com/}. 
		XTREE examines the historical record of code metrics for a project. If there is no evidence that changing code metric ``X'' is useful (e.g., lowering ``X'' reduces defect-proneness), then developers should be discouraged from wasting effort on that change.
	% 	\bi
% 	\item If there is no evidence in the historical record that changing ``X'' is useful;
% 	\item Then discourage developers from doing ``X''.
% 	\ei
	
	We use data mining to generate two oracles: a {\em primary
		change oracle} and a {\em secondary verification oracle}.
	By combining these two oracles,
	we can identify and validate useful
	code reorganizations. 
	
	We use 
	the XTREE cluster delta algorithm as the {\em primary change  oracle}.
	XTREE 
	explores the historical record of a project to find clusters of modules (e.g., files or binaries).
	It then proposes a ``minimal'' set of changes $\Delta$ that can move a software module $M$ from a defective cluster $C_0$ to another with fewer defects $C_1$ (so $\Delta$
	is some subset of $C_1 - C_0$). % To define ``minimal'', XTREE uses an entropy measure.
	
	The {\em secondary verification oracle} checks if the primary oracle is proposing
	sensible changes. We create the verification oracle using Random Forest~\cite{breiman2001random} augmented with SMOTE (synthetic  minority over-sampling technique~\cite{chawla2002smote}).
	In our framework,  learning
	the secondary oracle is   a {\em separate} task from that of learning the primary
	oracle. This  ensures that the verification oracle offers an independent
	opinion on the value of the proposed changes.
	
	An advantage to the XTREE cluster delta approach is that it avoids the 
% 	When these oracles were applied to defect data from
% 	open source object-oriented software systems, we discovered a 
	{\em conjunctive fallacy}.
	A common heuristic in the bad smell literature~\cite{erni96,bender99,Shatnawi10,Alves2010,hermans15} is: for all static code measures that exceed some threshold, make changes such that the thresholds are no longer exceeded. That is:
	\begin{equation}\label{eq:df}
		\scriptstyle
		\begin{array}{rl}
			\mathit{bad}    & = \left(a_1 > t_1 \right) \bigvee \left(a_2 > t_2\right) \bigvee    ...                             \\
			\mathit{better} & = \neg\;\mathit{bad} = \left(a_1 \le t_1 \right) \bigwedge \left(a_2 \le t_2\right)  \bigwedge  ... 
		\end{array}
	\end{equation}
	We say that the above definition of ``better'' is a conjunctive fallacy
	since it assumes that the best way to improve code is to decreases multiple code attribute measures to below $t_i$ in order
	to remove the ``bad'' smells. In reality, the associations between static code measures are more intricate, for example,
	such that {\em decreasing}  $a_i$ may necessitate {\em increasing} $a_j$.
	It is easy to see why this is so.
% 	A requirement for  code reorganization is that the new code supports the same functionality
% 	as before. 
	For example, Fowler recommends that the Large Class smell be addressed by the Extract Class or Extract Subclass refactoring~\cite{fowler99}. If we pull code out of a function since that function has grown too
	large, that functionality has to go somewhere else. Thus, when \textit{decreasing} lines of code in one module, we may \textit{increase} its coupling to another module. XTREE's identifies such associations between metrics, thus avoiding the conjunctive fallacy.
% 	recommendation is that if we decrease a module's lines of code  then in the reorganized
% 	code, the functionality is achieved via a communication between several different parts
% 	of the system. That is, {\em decreasing} the lines of code metric within a module {\em increases}
% 	that module's coupling metrics.
	
	\subsection{Research Questions}
	
	This paper  claims that (a)~XTREE is more useful than \eq{df} to identify code reorganizations, and (b)~XTREE recommends a small subset of static code measures to change and thus can be used to identify superfluous code reorganizations (i.e., reorganizations based on omitted
	static code measures). To evaluate these claims, we compared the performance of XTREE with three other methods for recommending code reorganizations~\cite{Shatnawi10,Alves2010,me12c} according to the following research questions:
	
% 	In support of these claims, 
% 	the rest of this paper is structured as follows. In Section~2, we offer some background on bad smells. In Section~3, 
% 	different methods are presented for determining when code does/does not have bad smell. One of those methods
% 	will be XTREE while the others will come from other researchers. Section~4 contains the methods used in this study, the results of which are shown in  . Those results will answer five research questions.
	
	{\bf  RQ1: Effectiveness}: According to the verification oracle, which of the methods is most accurate in recommending code reorganizations that result in reduced numbers of defective files? 
% 	is the best  change oracle for identifying what and how	code modules should be changed? 
To answer this question, we used data from five OO Java projects
	(Ivy, Lucene, Ant, Poi, Jedit). It was found that:
	\begin{lesson}
		XTREE is the most accurate oracle on how to change code modules in order to reduce the number of defective files.
	\end{lesson}
	
	{\bf RQ2: Succinctness}: Our goal is to critique and, possibly,
	ignore irrelevant bad smell detectors.  If the recommended changes are minimal (i.e. affect fewest attributes) then those changes
	will be easiest to apply and monitor. Which method recommends changes to the fewest
	code attributes?
	\begin{lesson}
		Of all the code change oracles studied, XTREE recommends changes to the fewest number of static code measures.
	\end{lesson}
	
	{\bf RQ3: Stopping}: Our goal is to discourage code reorganization based on changes that lack
	historical evidence of being effective. How effective is XTREE at identifying what {\em not} to change?
	\begin{lesson}
		In  any  project,  XTREE's  recommended  changes  to 1--4 
		of the  static code attributes.  Any bad smell defined in terms of the remaining 19 to 16 code attributes (i.e. most of them)
		would hence be deprecated.
	\end{lesson}
	
	{\bf RQ4: Stability}: Across different projects, how consistent are the changes recommended by our best change oracle?
	\begin{lesson}
		The direction of change recommended by XTREE (e.g., to LOWER lines of code while RAISING coupling) is stable across repeated runs of change oracle.  
	\end{lesson} 
	
	{\bf RQ5: Conjunctive Fallacy}:  
	Is it always  useful  to apply \eq{df}; i.e. make code better by   reducing the values of  multiple code attributes? We find that:
	\begin{lesson}
		XTREE usually recommends reducing lines of code (size of the modules).
		That said,  XTREE often recommends {\em increasing} the values of other static code attributes.
	\end{lesson} 
	Note that {\bf RQ3, RQ4, RQ5} 
	confirms the intuitions
	of the project managers that prompted this investigation. We find evidence that:
	\bi
	\item Code reorganizations that decrease multiple measures may not yield improvement, In fact,  programmers may need to {\em decrease} some measures while {\em increasing} others.
    \item In the studied projects, XTREE recommends changes to approximately 20\% of the code measures, and thus reorganizations based on the remaining 80\% are unlikely to provide benefit.
    \ei
%     one-fifth  
% 	of the measures were usually found within  change recommendations. That is, it is 80\%\footnote{Isn't it 20\% likely that you will find a useful measurement?} likely that
% 	that code reorganization based on reducing any one measurement picked at random will be useful.
%     \ei
% 	\bi
% 	\item
% 	Yes,  indeed, their programmers
% 	were wasting time on   performing code reorganizations  that  decreased multiple measures when, in fact,
% 	they should be trying to {\em decrease} some measures while {\em increasing} others.
% 	\item
% 	Across all studied projects, XTREE found that  around one-fifth  
% 	of the measures were usually found within  change recommendations. That is, it is 80\% likely that
% 	that code reorganization based on reducing any one measurement picked at random will be useful.
% 	\ei
	Consequently, we  recommend the following use case for  XTREE:
	\bi
	\item Before doing  code reorganization based on a bad smell report...
	\item ...check and discourage any code reorganization   for which there is no proof
	in the historical record that the change improves the code.
	\ei
	This use case described  applies to both manual code reorganizations proposed by developers
	as well as the code reorganizations conducted by automatic methods~\cite{mkaouer2015many}. That is, XTREE could optimize automatic
	code reorganization by discouraging reorganizations for useless goals.
	
	
	
	\subsection{Relationship to Prior Work }
	A four page preliminary repor\footnote{https://goo.gl/2In3Lr}t on the XTREE system has been presented previously~\cite{krishna2015actionable}. That short report offered case studies on only two  of
	the five data sets studied here. We greatly expand on this prior work by:
	\bi
	\item
	Evaluating XTREE's recommendations against recommendations from three others methods by researchers exploring bad smells.
	\item
	Evaluating if XTREE's and other methods' recommended changes were sensible using the {\em secondary verification oracle}
	\ei
	Only sections 3.2.1, 3.2.2, and two-fifths of the results in \fig{jur} contain material
	found in prior papers. 
	
	
	
	\section{Why Not Just Ask Developers to Rank Bad Smells?}\label{sect:prelim}
	
	Why build tools like XTREE to  critique proposed developer actions?
	Our answer, documented in this section, is that (1)~SE literature is not clear which bad smells are ignorable or  high priority and must be resolved.
	Also, (2)~developers themselves are unclear on what bad smells are most important to fix.
	
	
	Much research endorses code smells as a guide for
	code improvement (e.g., code reorganization or preventative maintenance). A recent literature review by Tufano et al.~\cite{Tufano2015}  
	lists dozens of papers on smell detection and repair tools. 
	Yet
	other papers cast doubt on the value of bad smells
	as triggers for code improvement~\cite{Mantyla2004,Yamashita2013,Sjoberg2013}. 
	As we show in this paper,  
	tools, text books, and developers disagree on what bad smells
	are important enough to hunt down and eliminate. Our findings
	are consistent with other research suggesting that universal bad
	smells that ignore project context are not useful to guide code reorganizations~\cite{Mantyla2004,Yamashita2013,Sjoberg2013}.
	
	If the SE literature is contradictory, why not ignore it and use domain experts (software engineers) to decide
	what bad smells to fix? We do not recommend this since developer {\em cognitive biases} can mislead them to
	assert that some things are important and relevant when they are not. 
	A widespread malaise in software engineering is that
	beliefs about software are rarely revised and hence may be
	inaccurate and 
	misleading~\cite{passos11,jorgensen09,mei15,me16phase,prem16}. Software developers may have strong views on many issues, including bad smells, but those views may be wrong for the current
	project.
	Software engineering is not the only field with this problem.
	For example, the medical profession applies many practices based on studies that have been disproved (a recent article in the Mayo Clinic Proceedings~\cite{prasad13} found 146 medical practices based on studies in year i, but which were reversed by subsequent trials within years i + 10). Even when the evidence for or against a treatment or intervention is clear, medical providers and patients may not accept it~\cite{aschwanden10}. Aschwanden~\cite{aschwanden15} warns that {\em cognitive biases} such as confirmation bias (the tendency to look for evidence that supports what you already know and to ignore the rest) influence how we process information.
	
	As in medicine, so too in software engineering.
	According to Passos et al.~\cite{passos11},  developers often  assume that the lessons they learn from a few past projects are general to all their future projects. They comment ``past experiences were taken into account without much consideration for their context''~\cite{passos11}.  J{\o}rgensen \& Gruschke~\cite{jorgensen09} offer a similar warning. They report that the supposed software engineering ``gurus'' rarely use lessons from past projects to improve their future reasoning and that such poor past advice can be detrimental to new projects.~\cite{jorgensen09}. Other studies have shown some widely-held views are   now questionable given new evidence:
	
	\bi
	% COMMENT FROM LL: This bullet is weak. Maybe goto isn't used in an unrestricted manner because of Djikstra's warning.
	% \item
	% Nagappan et al. analyzed~\cite{mei15} 
	% how   {\tt goto}  was used  in 11,000 Github repositories. 
	% They found that developers
	% rarely used {\tt goto} in the unrestricted manner that alarmed Dijkstra in his  1968
	% memo ``Go to statement considered harmful''~\cite{Dijkstra68}.
	\item
	In other work~\cite{me16phase} with the Software Engineering Institute (SEI), we have revisited
	the ``phase delay'' truism that {\em the cost of fixing an error dramatically increases the longer it is in the system}. 
	We found no evidence for such large   phase delay effect in modern
	software (in 171 software projects shepherded
	by the SEI, 2006-2014) possibly since it has been  heavily mitigated by $21^{st}$ century software development languages, tools and development practices. 
	Whatever the reason, the main point  is that phase delay is a widely
	held belief, despite little recent evidence to support it.
	\item
	Devanbu et al. examined responses from 564 Microsoft software developers from around
	the world, they found that  ``(a)~programmers do indeed have very
	strong beliefs on certain topics; (b)~their beliefs are primarily formed
	based on personal experience, rather than on findings in empirical
	research; (c)~beliefs can vary with each project, but do not necessarily
	correspond with actual evidence in that project''~\cite{prem16}.
	\ei
	Devanbu et al. further  comment that ``programmers give personal experience
	as the strongest influence in forming their opinions''. This is troubling
	especially given the comments from Passos and J{\o}rgensen et al.~\cite{passos11,jorgensen09} about how quickly practitioners form, freeze, and rarely revisit those opinions.
	
	
	
	\input{refactor}
	
	If the above remarks hold true for bad smells, then we would expect
	to see much disagreement on which bad smells are important and relevant
	to  a particular project. This is indeed the case.
	The first column of \fig{smells} 
	lists  commonly mentioned bad smells and comes from Fowler's 1999 text~\cite{fowler99} and a subsequent 2005 text by Kerievsky that is widely cited~\cite{Kerievsky2005}.
	The other
	columns show data from other studies on which bad smells matter most.
	The columns marked as Lanza'06 and Yamashita'13 are from peer reviewed literature. The column marked SonarQube is a popular open source
	code assessment tool that includes detectors for six of the bad smells
	in column one. 
	The {\em developer survey} (in the right-hand-side column) shows the results of an hour-long whiteboard sessions with a group of 12 developers from a Washington
	D.C. web tools development company. Participants
	worked in a round robin manner to rank the bad smells they thought were
	important (and any disagreements were discussed with the whole group).
	Amongst the group, there was  some
	consensus on  the priority of which bad smells to fix
	(see the annotations VH=very high,
	H=high, L=low, VL=very low, and ``?''= no consensus).  
	
	
	
	\input{jur_char}
	
	A  blank cell in \fig{smells}
	indicates where   other work omits 
	one of the bad smells in column one. 
	Note that most of the cells are blank, and that the studies omit the majority of the Fowler bad smells.
	SonarQube has no detectors for many of the column one bad smells.
	Also, nearly half the Yamashita list of bad smells
	does not appear in column 1. The eight numbers
	in the  Yamashita'13 column show the rankings for the bad smells 
	that overlap with Fowler and Kerievsky; Yamashita also discussed other smells not covered in Fowler'99~\cite{fowler99}.
	
	
	Two of the studies in \fig{smells} offers some comments on the relative importance
	of the different bad smells. Three of the bad smells listed in the top half of the Yamashita'13 rankings also score very high in the developer survey. Those three were {\em duplicated code, large class}, 
	and {\em long method}. 
	Note that this agreement also means that the
	Yamashita'13 study and the developer survey   
	believe
	that very few  code smells are   high priority issues
	requiring code reorganization. 
	
	In summary, just because one developer strongly believes in the importance of a bad smells does not mean that belief transfers to other developers or projects.
	Developers can be clever, but their thinking can also be distorted
	by cognitive biases.
	Hence, as shown in \fig{smells}, developers, text books, and tools 
	can disagree on which bad smells are important.
	Special tools are needed to assess their beliefs, for example, their beliefs in
	bad smells.  
	
	
	\section{Learning Bad Smell Thresholds}\label{sect:bst}
	
	Having made the case for automated, evidence-based support for assessing bad smells,
	this section reviews different ways for building those tools (one of those tools,
	XTREE, will be our recommended {\em primary change oracle}).
	Later in this paper, we describe  a {\em secondary verification oracle}
	that checks the effectiveness of the proposed changes.
	
	The SE literature offers two ways of learning bad smell thresholds.
	One approach relies on 
	{\em outlier statistics}~\cite{erni96,bender99}. This approach
	has been used   by Shatnawi~\cite{Shatnawi10}, Alves et al.~\cite{Alves2010}
	and Hermans et al.~\cite{hermans15}.
	Another approach is 
	based on {\em cluster deltas} that we developed
	for   Centroid Deltas~\cite{me12c} and 
	use here for XTREE. 
	These two approaches are discussed below. 
	
	\subsection{Outlier Statistics}
	
	The outlier approach assume that unusually large measurements indicate risk-prone code.
	Hence, they generate one bad smell threshold for any metric
	with such an ``unusually large'' measurement. 
	The literature lists several ways to define ``unusually large''.
	
	
	
	
	\subsubsection{Enri \& Lewerentz}
	Given classes described with the  code metrics of \fig{ck},
	Enri and Lewerentz~\cite{erni96} found the   mean $\mu$ and the standard deviation $\sigma$
	of each
	code metrics. Their definition of problematic outlier was any code
	metric with a measurement greater than $\mu+\sigma$.
	Shatnawi and Alves et al.~\cite{Shatnawi10,Alves2010} depreciate
	using $\mu+\sigma$ since it does not consider the fault-proneness of classes when the thresholds are computed. Also, the method lacks  empirical verification.
	
	\subsubsection{ Shatnawi}\label{sect:shatnawi}
	Shatnawi~\cite{Shatnawi10}'s preferred alternative to $\mu+\sigma$
	is to use the VARL method (Value of Acceptable Risk Level) proposed by Bender~\cite{bender99} in his epidemiology studies.  This approach uses two
	constants to compute the thresholds which, following Shatnawi's guidance, we set to
	$p_0=p_1=0.05$.
	
	
	\input{xtree_tut.tex}
	VARL encodes the defect count
	for each class as 0 (no defects known in class) or 1 (defects known in class).
	Univariate binary logistic regression is applied to learn three coefficients:  
	$\alpha$ is the intercept constant;
	$\beta$ is the coefficient for maximizing log-likelihood;
	and $p_0$ 
	measures   how well this  model predicts for  defects. A univariate logistic regression was conducted comparing metrics to defect counts. Any code metric with $p>0.05$ is ignored as being a poor defect predictor. Thresholds are then learned from the surviving metrics $M_c$ using
	an equation proposed by Bender:
	$$ \mathit{bad\; smell\; if\;} M_c > VARL$$
	Where,
	\begin{equation}
		VARL = p^{-1}(p_0) =  \frac{1}{\beta }\left( {\log \left( {\frac{{{p_1}}}{{1 - {p_1}}}} \right) - \alpha } \right) 
	\end{equation}
	
	
	\subsubsection{ Alves et al.}
	Alves et al.~\cite{Alves2010} propose another approach 
	to finding thresholds that  utilizes the underlying statistical distribution and scale of the metrics. 
	Metric values for each class are weighted according to the source lines of code (SLOC) of the class. All the weighted metrics are then normalized by the sum of all weights for the system. 
	The normalized metric values are ordered in an ascending fashion (this is equivalent to computing a density function, in which the x-axis represents the weight ratio (0-100\%), and the y-axis the metric scale).
	Alves et al. then select a percentage value (they suggest 90\%) which represents the ``normal'' values for metrics. The metric threshold, then, is the metric value for which 90\% of the classes fall below. The intuition  is that the worst code has outliers beyond 90\% of the normal code measurements. Hermans et al.~\cite{hermans15} used the
	Alves et al. method in their  2015 paper on
	exploring bad smells. For consistency with Shatnawi, we explore the correlation between the code metrics and the defect counts and  reject code metrics that are poor predictors of defects (i.e.   those  with $p > 0.05$).\footnote{I still do not understand the reference to Shatnawi at the end of this paragraph on Alves. Are you doing some correlation using the Alves criteria?}
	
	\subsubsection{Discussion of Outlier Methods}\label{sect:disc}
	The advantage of the outlier-based
	approaches is that they are simple to implement, but the approaches have   two  major disadvantages. 
	First, they are {\em verbose}. A threshold can be calculated for every metric -- so, which one should the developers focus on changing? Without a means for prioritizing the  thresholds and metrics against one another, developers may have numerous or conflicting recommendations on what to improve. Second, the outlier approaches suffers from {\em conjunctive fallacy}  discussed in the introduction. That is, while	they propose thresholds for many code metrics
	individually, they make no comment on what minimal metrics need to be changed at the same time (or whether or not those changes lead to minimization or maximization of static code measures).
	
	
	\subsection{Cluster Deltas}
	
	Cluster deltas are a general method
	for learning {\em conjunctions} of changes
	that need to be applied at the same time. 
	This approach works as follows:
	\begin{itemize}
		\item Cluster the data. 
		\item Find
		neighboring clusters $C_+,C_-$ where $C_+$ has more examples of defective
		modules than $C_-$;
		\item Compute the  delta   in code metrics between the clusters using \mbox{$\Delta = C_- - C_+ = \left\{\delta|\delta\in C_-, \delta \notin C_+\right\}$}, i.e.
		{\em towards} the cluster with lower defects;
		\item The set $\Delta$ are changes needed in defective modules of $C_+$ to
		make them more like the less-defective modules of $C_-$
	\end{itemize}
	Note that $\Delta$ is a conjunction of  recommendations.
	Since it is computed
	from neighboring clusters, the examples contain similar distributions and $\Delta$ respects the naturally occurring constraints in the data. For example,
	given a bad smell pertaining to large methods,   $\Delta$   will not  suggest lowering lines of code
	without also increasing a coupling measure. 
	Cluster deltas are used in CD~\cite{me12c} and XTREE.
	
	
	
	\subsubsection{CD}\label{sec:cdcd}
	Borges and Menzies first proposed the CD centroid delta approach to
	generate {\em conjunctions} of code metrics
	that need to be changed at the same time
	in order to reduce defects~\cite{me12c}.
	CD uses the WHERE clustering algorithm developed by the
	authors for a prior application~\cite{localvsglobal}.
	Each cluster was then replaced by its centroid
	and $\Delta$ was calculated directly from the difference
	between code metrics values between one centroid
	and its nearest neighbor.
	
	
	One drawback with with CD is that it is {\em verbose}
	since
	CD   recommended changes to all code
	metrics with different values in those two centroids. 
	This makes it hard to use CD to   critique and prune away bad smells. Further, CD will be shown to be
	not as effective
	in proposing changes to reduce defects as XTREE.
	
	\subsubsection{XTREE: Mining Project History for Defect-Proneness Attributes}
	
	XTREE  is a cluster delta algorithm
	that avoids the problem of verbose $\Delta$s.
	XTREE is our {\em primary change oracle} that makes recommendations 
	of what changes should be made to code modules.
	Instead of reasoning over cluster centroids,
	XTREE utilizes a decision tree learning approach
	to find the fewest differences between clusters of examples.
	
	
	XTREE uses a multi-interval discretizer based on an iterative dichotomization scheme, first proposed by Fayyad and Irani~\cite{fi}. This method converts the values for each code metric into a small number of nominal ranges. It works as follows:
	\begin{itemize}
		\item A code metric is split into $r$ ($r=2$)\footnote{Why do you select r=2?} ranges, each range is of
		size $n_r$ and is associated with a set of defect counts $x_r$ with standard deviation
		$\sigma_r$. 
		\item The best split for that range is the one that minimizes the expected value of the
		defect variance, after the split; i.e. $\sum_r\frac{n_r}{n}\sigma_x$ (where $n=\sum_r n_r$). 
		\item This discretizer then recurses on each part of the split to find other splits in a recursive fashion. As suggested by Fayyad and Irani, minimum description length (MDL) is used as a termination criterion for the recursive partitioning.
	\end{itemize}
	
	When discretization finishes, each code metric $M$ has a 
	final expected value $M_v$ for the defect standard deviation 
	across all the discretized ranges of that metric.
	Iterative dichomization sorts the metrics by $M_v$
	to find the code metric that best splits the data i.e., the code metric with smallest $M_v$. 
	
	A decision tree is then constructed on the discretized metrics. The metric that generated the best split forms the root of the tree with its discrete ranges acting as the nodes. 
	
	When all the metrics are arranged this way, the process is very similar to a hierarchical clustering algorithm that groups together code modules with similar defect counts and some shared ranges of code metrics.
	For our purposes, we score each cluster found in this way according
	to the percent of classes with known defects. For example, the last line of \fig{xtree_samp} shows a tree leaf with 75\%
	defective modules.
	
	\fig{xtree_samp} offers
	a small example of how XTREE builds
	$\Delta$ by comparing branches that lead to leaf clusters
	with different defect percentages. In this example, assume a project with a table of code metrics data describing its classes in the form of \fig{ck}. After code inspections and running test cases or operational
	tests, each such class is augmented with a defect count.
	Iterative dichomization takes that table of data and, 
	generates the tree of \fig{xtree_samp}.
	
	Once the tree is built, a class with code metric data is passed into the tree and evaluated down the tree to a leaf node (see the \textcolor{orange}{{\bf orange}} line in \fig{xtree_samp}).
	XTREE then looks for a nearby leaf node with a lower defect
	count (see the \textcolor{green}{{green}} line in \fig{xtree_samp}). For that evaluated class, XTREE proposes a bad smell
	thresholds that is  the difference between 
	\textcolor{green}{{green}} and \textcolor{orange}{{\bf orange}}. 
	
	
	\subsubsection{XTREE: Recommending a Code Reorganizations}
	
	Using the training data construct a decision tree as suggested above.
	
%	uses  
%	iterative dichomization to
%	divide  $N$ code modules  into  clusters of
%	size $\sqrt{N}$.
	
	Next, for each test code module, 
	find $C_+$ as follows: take each test, run it down the decision tree to find a leaf in the decision tree that mostly matches the test case.  
	After that,	find $C_-$ as follows:
	\begin{itemize}
		\item Starting at the  $C_+$ leaf, ascend $lvl\in \{0,1,2...\}$ tree levels;
		\item Identify {\em sibling} leaves; i.e. leaf clusters that can be reached from level $lvl$ that are not same as {\em current $C_+$};
		\item Find the {\em better} siblings; i.e. those with defect proneness 50\% or less than that of $C_+$ (e.g., if defect-proneness of $C_+$ is 70\%, find the nearest sibling with defect proneness $\leq$ 35\%). If none found, then repeat for $lvl += 1$. Also, return \texttt{nil} if the new $lvl$ is above the root. 
		\item  Set $C_-$ to the  {\em closest} better sibling where distance is measured between the mean centroids of that sibling and {\em $C_+$}
		\ei
		Now find $\Delta = C_- - C_+$  by reflecting
		on the set difference between  conditions in the decision tree branching from $C_+$ to $C_-$. To find that delta,
		for discrete attributes, delta is the value of the {\em desired}; for numerics  expressed as ranges, the delta could be any value that lies between $\left[ LOW, HIGH\right]$ in that range (for example, a random number selected from the low and high boundaries of the that range.\footnote{Please justify why you select a random value. random selection. My footnote from an earlier version has not been addressed and at least needs to be a limitation of XTREE: defect proneness. PRevious footnote: This could lead to conflicting recommendations in different runs of XTREE, right? Consider Figure 3. Suppose C+ has cbo=(27.00,157.00):1.00. XTREE ascends one level and descends to the leaves of next subtree. XTREE suppose that C" is a leaft where cbo=(1.00,50:00):0.15 (instead of cam as in Figure 3). By selecting something random in the range, you could recommend either an increase or a decrease to CBO. I would think that this is an instance where XTREE should say 'I don’t know what to do with CBO, but you should improve LCOM.' Alternately, if you took the median value (instead of random) and compared, you could at least recommend a direction, i.e., ``the best benefits come from REDUCING lcom while maintaining a LOWER cbo'')}
		
		Note that XTREE's recommendation does not exhaustively search the tree for the change that reduces defect proneness the most, but rather finds the nearest sibling. This is by design. This design allows XTREE to a) provide recommendations quickly, and b) to recommend changes to a small number of attributes.
		
		\input{prediction_stats.tex}      
		\section{Setup}
		The previous section proposed numerous methods for detecting bad smells that need
		to be resolved. This section offers a way to evaluate them as follows:
		\bi
		\item Use each method as a {\em primary change oracle} to recommend how code should be changed in our test data set (Section~\ref{sect:tesd}).
		\item Apply those changes. (This is emulated by changing the code metrics in order that all the $\Delta$'s are addressed.)
		\item Run a {\em secondary verification oracle} to assess the defect-proneness of the changed code.
		\item Sort the change oracles on how well they reduce defects as judged by the verification oracle.
		\ei
		Using this, we can address the research questions discussed in the introduction.
		
		{\bf RQ1: Effectiveness: } Which of the methods defined in Section 3 is the best change oracle for identifying what and how code modules should be changed? 
		we will assume that developers
		update and reorganize their code until the bad smell thresholds are not violated.
		This code reorganization will start with some {\em initial} code
		base that is changed to a {\em new} code base. For example, if the bad smell is \mbox{{\em loc $>100$}} and a 
		code module has 500 lines of code, we reason
		optimistically that we can change that code metric
		to 100.  Using the secondary verification oracle,  we then predict the
		number of defective files in {\em new}. 
		
		We compare $d_+$, the number of defective files in the \textit{initial} code base to $d_-$, the number of defective files in the \textit{new} code base.
		We evaluate the performance of XTREE, CD, Shatnawi, and Alves change oracles in methods. The best change oracle is the one that maximizes
		\begin{equation}\label{eq:diff}
			\mathit{improvement} = 100* \left(1 - \frac{ d_- }{ d_+}\right)
		\end{equation}
		
		
		{\bf RQ2: Succinctness: } Which of the Section 3 methods recommended changes to the fewest code attributes?
		To answer this question, we will report the frequency at which different attributes are selected in repeated runs of our oracles.
		
		{\bf RQ3: Stopping: }    How effective is XTREE at offering   ``stopping points'' (i.e. clear guidance on what {\em not} to do)?   
		To answer this point, we will report how often XTREE's recommendations {\em omit} a code attribute. 
		Note that the {\em more often} XTREE omits an attribute, the more likely it is {\em not} to endorse a bad smell based on the omitted
		attributes.
		
		{\bf RQ4: Stability:} Across different projects, how variable are the changes recommended by XTREE?   To answer this
		question, we conduct a large scale ``what if'' study that reports all the possible recommendations XTREE might make.
		We then count how often attributes are {\em not} found in the recommendations arising from this ``what if'' study.
		
		{\bf RQ5: Conjunctive Fallacy:} Is  it  always  useful  to  apply \eq{df}; i.e.   make  code  better  by  reducing  the  values  of multiple code attributes? To answer this question, we will look at the {\em direction of change} seen in the {\bf RQ4} study; i.e.
		how often XTREE recommends decreasing or increasing a static code attribute.
		
		
		
		
		
		\subsection{Test Data}\label{sect:tesd}
		
		To explore these research questions, we used data from Jureczko et al.'s collection of object-oriented Java systems~\cite{jureczko10}. To access that data, go to   git.io/vGYxc.
		The Jureczko data records the number of known defects for each class using a post-release bug tracking system. The classes are described in terms of nearly two dozen metrics included in the Chidamber and Kemerer metric suite, such as number of children (noc), lines of code (loc), etc. For details on the Jureczko code
		metrics, see  \fig{ck}. For details on the rows and versions of that data, see the left-hand-side columns of \fig{jur}.
		
		
		
		
		\subsection{Building the Secondary Verification Oracle}
		\label{sect:eval}
		
		As mentioned in the introduction, our proposed framework has two oracles:
		a primary change oracle (XTREE) and the secondary verification oracle described in this section.
		
		It can be difficult  to judge the  effects of removing bad smells.
		Code that is reorganized cannot be assessed just by a rerun of the the test
		suite since such reorganizations may not change the system behavior (e.g., refactorings). 
		%Further, a test suite may not be available or the compilation environment reproducible. 
		% waiting to observe 
		% code refactorings is meant to
		% preserve the behavior of the system (i.e. the same tests
		% should pass before and after the refactoring). 
% 				Also, it many not be practical
% 		to assessing code reorganizations by making the
% 		code reorganizations,  then seeing what happens to that code for the rest
% 		of the project. In a test-driven agile project, as
% 		SCRUM meetings change what stories are implemented next,
% 		the operational profile that lead to defects in the old
% 		code may not be repeated in future executions. 
		
		To resolve this  problem, SE researchers such as 
		Cheng et al.~\cite{Cheng10}, O'Keefe et al.~\cite{OKeeffe08,OKeeffe07},
		Moghadam~\cite{Moghadam2011} and Mkaouer et al.~\cite{Mkaouer14}
		use a {\em secondary verification oracle} that is learned separately
		from the primary oracle. The verification oracles assesses
		how defective is the code before and after some
		code reorganization. 
		For their second oracle,
		Cheng, O'Keefe, Moghadam and  Mkaouer et al. use the QMOOD hierarchical
		quality model~\cite{Bansiya02}.
		A shortcoming of QMOOD
		is that quality models learned from other projects
		may perform poorly when applied to new projects~\cite{localvsglobal}.
		Hence, for this study, we  eschew
		older quality models like QMOOD. Instead, we use
		Random Forests~\cite{Breiman2001} to learn defect predictors
		from OO code metrics.
		Unlike QMOOD, the predictors 
		are specific to the project.
% 		, and the measurements can be reconstructed and the predictors rebuilt for other projects.
		
		Random Forests are a decision tree learning method but
		instead of building one tree, hundreds are built using
		randomly selected subsets of the data. The final predictions
		come from averaging the predictions over all the trees.
		Recent studies endorsed the use
		of  Random Forests for  defect prediction~\cite{lessmann}.
		
		\fig{j} shows  our experiments with Random Forests and
		the Jureczko data. The goal is to build a verification oracle based on Random Forest that accurately distinguishes between defective and non-defective files based on code metrics. Given $V$ released versions, we test on version $V$ and train on the available data from $V-1$ earlier releases. Note the   \colorbox{lavenderpink}{three bottom rows}   marked with $\times$: these contain predominately defective classes (two-thirds, or more).  It is hard to build a model that distinguishes non-defective files from defective files in these data sets due to the high percentage of defective file examples. 
		
		We use Boolean classes in the  Jureczko data to identify the presence or absence of defects: \texttt{True} if defects \textgreater 0; \texttt{False} if defects = 0. The quality of the predictor is measured using (a) the  probability of detection \textit{pd} (i.e., recall):  the percent of faulty classes in the test data detected by the {\em predictor}; and (b) the  probability of false alarm \textit{pf} (i.e., false positives): the percent of non-fault classes that are {\em predicted} to be defective.
		
		The ``untuned'' columns of \fig{j}
		show a preliminary experiment using Random Forest with its ``off-the-shelf'' tuning of 100 trees per forest.  
		The forests were built from training data and applied to test data
		not seen during training.  In this
		study, we called a data set ``usable'' if   Random Forest was able to classify the instances with a performance threshold of $\mathit{pd}\ge 60 \wedge \mathit{pf} \le 30$\% (determined from standard results in other publications~\cite{me07b}). Note that no  data set meet
		that criteria.
		
		The ``tuned'' columns of \fig{j} show that we can salvage some of the data sets. We applied both the SMOTE algorithm and differential evolution to improve the performance of the classifier. Pelayo and Dick~\cite{pelayo07} report that defect prediction is improved by SMOTE~\cite{Chawla2002}; i.e. an over-sampling of minority-class examples and an under-sampling of majority-class examples. Fu et al.~\cite{fu:ase15} report that parameter tuning with differential evolution~\cite{storn97} can quickly explore the tuning options of Random Forest to find better settings for the size of the forest, the termination criteria
		for tree generation, and other settings. We note that SMOTE-ing and
		parameter tunings were applied to the training data only and not to the test data.
		
		The rows \colorbox{celadon}{marked with a $\star$} in \fig{j} show data sets whose performance was improved remarkably by these techniques. For example, in {\em poi}, the recall increased by 4\% while the false alarm rate dropped by 21\%. However, as might have been expected, we could not salvage the data sets in the  three bottom rows.
		
		We eliminate the data sets for which we could not build an adequately performing Random Forest classifier with $\mathit{pd}\ge 60 \wedge \mathit{pf} \le 30$\%. Thus, our analysis uses the {\em jedit, ivy, ant, lucene} and {\em poi} data sets for evaluating recommended changes.
		
		
		
%		\subsection{Experiment}
%		Describe what you actually did, including: 
%		\bi
%		\item what metrics you used to identify refactorings using the 4 methods. Also, what is your data here? Is it the rows and columns of CK/OO metrics for each file?
%		\item how you applied the recommended changes from the 4 methods to the data 
%		\item how you chose to handle the ``conjuctive fallacy' of non-CD, non-XTREE methods
%		\item What are you considering correct, i.e., if the change to the code metrics yielded a non-defective (i.e., \# defects = 0) file, then the refactoring is good. 
%		\item how you aggregated the results from your 40 runs for each method
%		\ei
%		
		
		\subsection{Statistics}
		
		
		We use 40 repeated runs for each code reorganization recommendation method for each of the five data sets (we use 40 since that is  more than the 30 samples  needed to satisfy the central limit theorem). Each code organization method is trained on versions $1...N-1$ of $N$ available versions in each data set.
		Each run collects the improvement scores defined in \eq{diff}: the reduction in the percentage of defective files as identified by the verification oracle after applying the recommended code metric changes.
		
		We use multiple runs with different random number seeds since two of our methods use some random choices: CD uses the  stochastic WHERE clustering algorithm~\cite{localvsglobal}
		while XTREE non-deterministically picks thresholds randomly from
		the high and low boundary of a range. 
		Hence, to compare all
		four methods, we must run the analysis many times. 
		
		
		
		\input{results_jur.tex}
		
		To rank these 40 numbers collected from CD, XTREE, Shatnawi, and Alves et al., we use the Scott-Knott test recommended by Mittas and Angelis~\cite{mittas13}. 
		Scott-Knott is a top-down clustering approach used to rank different
		treatments. If that clustering finds an interesting division of the data, then
		some statistical test is applied to the two divisions to check if they
		are statistically significant different. If so, Scott-Knott recurses
		into both halves.
		
		To  apply Scott-Knott,
		we
		sorted a list of  $l=40$ values of \eq{diff} values found in  $ls=4$ different methods. 
		Then, we split $l$ into sub-lists $m,n$ in order to maximize the expected value of differences in the observed performances before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$: \[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]
		We then apply a apply a statistical hypothesis test $H$ to check
		if $m,n$ are significantly different  (in our case, the conjunction of A12 and bootstrapping). If so, Scott-Knott recurses on the splits. In other words, we divide the data if \textit{both} bootstrap sampling and effect size test agree that a division is statistically significant (with a confidence of 99\%) and not a small effect ($A12 \ge 0.6$).
		For a justification of the use of non-parametric bootstrapping, see Efron \& Tibshirani~\cite[p220-223]{efron93}. For a justification of the use of effect size tests see Shepperd\&MacDonell~\cite{shepperd12a}; Kampenes~\cite{kampenes07}; and Kocaguenli et al.~\cite{Kocaguneli2013:ep}. These researchers warn that even if a hypothesis test declares two populations to be ``significantly'' different, then that result is misleading if the ``effect size'' is very small. Hence, to assess the performance differences we first must rule out small effects using A12, a test   recently endorsed by Arcuri and Briand at ICSE'11~\cite{arcuri11}.
		
		The Scott-Knott  results are presented in the form of line diagrams like those shown on the right-hand-side of \fig{jur}.
		The black dot shows the median \eq{diff} values and the horizontal lines stretches 
		from the 25\textsuperscript{th} percentile to the 75\textsuperscript{th} percentile (the inter-quartile range, IQR). 
		As an example of how to read this table, consider the {\em Ant}
		results. Those rows are  sorted on the median values of each method. Note that all the methods have \eq{diff} \textgreater $ 0\%$; i.e. all these methods reduced the expected value of the performance score while XTREE achieved the greatest reduction (of 56\% from the original value).
		These results table has a  left-hand-side  {\bf Rank} column, computed using the
		Scott-Knott test described above. In the {\em Ant}
		results, XTREE is ranked the best, while CD is  ranked   worst.
		
		\input{rq2.tex}
		\section{Results}
		
		
		\begin{figure*}[!t]
			\centering
			\includegraphics[width=\linewidth]{figs/changes01.png}
			\caption{Results  from XTREE.
				While \fig{counts} are the {\em number} of times a code metric was changed,
				this  figure shows the {\em magnitude} each code metric was changed. Each vertical bar
				marks the 27,50,75\textsuperscript{th} percentile change seen in 40 repeats.
				All numbers are ratios of initial to final values.
				All bar regions marked in gray show {\em increases}.
				The interesting feature of these results are that many
				of the changes proposed by XTREE require {\em increases}
				(this puzzling observation is explained in the text).}
			\label{fig:changes}
		\end{figure*}
		
		
		
		\subsection{RQ1: Effectiveness}
		
		{\em Which of the methods defined in Section 3 is the best change oracle for identifying what and how code modules should be changed? }
		
		\fig{jur} shows the comparison results.  
% 		The Alves and Shatnawi
% 		entries show the net effects of applying \eq{df} where the $t_i$ values were found by Alves and Shatnawi.
% 		In this part of the  analysis,
% 		we applied Scott-Knott to all the \eq{diff} values
% 		seen when code metrics were changed
% 		according to the  Alves and Shatnawi
		Two data sets are very responsive to defect reduction suggestions:
		Ant, and Ivy (both of which show best case improvements over 50\%).
		The  expected value of defects  
		is changed less in Jedit. This data sets' results
		surprisingly uniform; i.e.   all methods
		find the same ways to reduce the expected number of
		defects.   For an explanation of the Jedit uniformity, see \tion{inc}\footnote{Broken reference?}.
		
		Two data sets are not very responsive to defect reduction:
		Poi and Lucene. The reason for this can be see in \fig{j}:
		both these data sets contain more than 50\% defective modules.
		In that space, all our  methods lack a large sample of
		defect-free examples. 
		
		Also consider the relative
		rank of the different approaches,
		CD and Shatnawi usually  perform comparatively worse while  XTREE gets top ranked position the most
		number of times. That said, Alves sometimes beats XTREE (see Ivy)
		while sometimes it ties (see Jedit).
		
		In summary, our {\bf Result1} is  that, of the change oracles studies here,
		XTREE is the best oracle on how to change code modules in order to reduce defects in our data sets.
		
		\input{all_changes.tex}
		
		\subsection{RQ2: Verbosity}
		
		{\em Which of the Section 3 methods recommended changes to the fewest code attributes?}
		
		\fig{counts} shows the frequency with which the methods
		recommend changes to specific code metrics.
		Note that XTREE proposes thresholds to
		few code metrics compared to the other approaches. 
		
		% COMMENT From LL: This is such a strange way of putting this lesson, and you haven't actually evaluated this.
		% 
		
		Hence, our {\bf Result2} is that, of all the code change oracles studied here, XTREE recommended far fewest changes to static code measures.
		Note only that,   combining  \fig{jur} with \fig{counts}, we   see that
		even though XTREE proposes changes to far fewer code metrics, those few
		code metrics are usually just as effective (or
		more effective) than the multiple
		thresholds
		proposed by CD, Shatnawi or Alves.  That is, XTREE proposes
		{\em fewer} and better thresholds than the other approaches studied here.
		
		
		
		\subsection{RQ3: Stopping}
		
		{\em  How effective is XTREE at offering   ``stopping points'' (i.e. clear guidance on what not to do)?}
		
		The {\bf RQ2} results showed that XTREE's recommendations are small in a {\em relative sense}; i.e. they are 
		relatively smaller than the other methods studied here.
		Note only that, but XTREE's recommendations are small in an {\em absolute sense}.
		Consider all frequency at which  \fig{counts} values for XTREE that are over 33\%; i.e. in at a third of our  repeated runs, XTREE mentioned
		a code attribute. For Ant, Ivy, Lucene, Jedit, and Poi, those frequencies
		are  \mbox{3, 3, 3, 4, 1, 2} respectively (out of twenty). This means that, usually, XTREE omits references to \mbox{17,17,17,16,19,18} static
		code attributes (out of 20). 
		Any code reorganization based on a bad smell detector that uses  these omitted code attributes could hence  be stopped.
		
		Hence our {\bf Result3} is that, in a any  project,  XTREE's  recommended  changes  only  mentions one to four 
		of the  static code attributes.  Any bad smell defined in terms of the remaining 19 to 16 code attributes (i.e. most of them)
		would hence be deprecated.
		
		
		\subsection{RQ4: Stability}
		
		{\em Across different projects, how variable are the changes recommended by our best change oracle? }
		
		
		\fig{counts} counted how {\em often} XTREE's recommendations mentioned a static code attribute.
		\fig{changes}, on the other hand, shows the {\em direction} of XTREE's recommended change:
		\bi
		\item Gray bars show  an  {\em increase} to a static code measure;
		\item White bars shows a   {\em decrease} to a static code measure;
		\item Bars that are all white or all gray indicate that in our 40 repeated runs, XTREE recommended changing an attribute the same way, all the time.
		\item Bars that are mixtures of white and gray mean that, sometimes, XTREE makes different recommendations about how to change a static
		code attribute.
		\ei
		Based on \fig{changes}, we say that {\bf Result4} is that the direction of change recommended  XTREE is  very stable repeated runs of the program  (evidence:
		the bars are mostly the same color).
		
		\fig{changes} also comments on the inter-relationships between static code attributes. Note that while some measures in
		\fig{changes} are decreased, many are increased.  
		For example, consider the {\em Poi} results
		from \fig{counts} that recommends decreasing {\em loc}
		but making large increases to {\em cbo} (coupling between
		objects). Here, XTREE is advising us to break up
		large classes class by into services
		in other classes. Note that such a code reorganization will, by
		definition, increase the coupling between objects. 
		Note also that such increases  to reduce
		defects would never be proposed by the outlier methods
		of Shatnawi or Alves since their core assumption is that bad
		smells arise from unusually large code metrics.
		
		\subsection{RQ5: Conjunctive Fallacy}
		
		{\em  Is  it  always  useful  to  apply \eq{df};  i.e.   make  code  better  by  reducing  the  values  of multiple code attributes?}
		
		In  \fig{changes}, the bars are colored both white and gray; i.e. XTREE recommends {\em decreasing} and {\em increasing} 
		static attribute values. That is, always decreasing static code measures (as suggested by \eq{df}) is {\em not} recommended by XTREE.
		
		To explore this point further, we conducted the following ``what-if'' study. Once XTREE builds its trees with its leaf clusters then:
		\be
		\item For all clusters $C_0$, $C_1$ where the percent of defects in $C_0$ is greater than $C_1$...
		\item For all attribute measures that have a statistically significantly different distribution  between $C_0$ and $C_1$...
		\item Report the direction of the change.
		\ee
		Note that the changes found in this way are somewhat more general than the above results
		since they were  limited
		to comments on the test set given to the program. The above three points comments on the direction of {\em all possible changes}
		that XTREE could ever report
		
		
		Figure ~\ref{fig:multcomp} shows the results of this ``what if'' analysis. As might be expected, the recommendation is to
		always reduce lines of code (loc). But for the other attributes, there are many recommendations to increase those values.
		Hence, {\bf Result5} is that while XTREE always  recommends reducing loc,
		it also   often recommends increasing the values of other static code attributes.   
		

\section{Reliability and Validity of Conclusions}
\label{sect:valid}



The results of this paper are biased by our choice of code reorganization 
goal (reducing defects) and our choice of measures collected from software 
project (OO measures such as depth of inheritance, number of child classes, 
etc). That said, it should be possible extend the methods of this paper to other 
kinds of goals (e.g. maintainability, reliability, security, or the knowledge sharing
measures) and other kinds of 
inputs (e.g. the process measures favored by Rahman, 
Devanbu et al.~\cite{Rahman2013}) 

\subsection{Reliability}
Reliability refers to the consistency of the results obtained
from the research. It has at least two components: internal
and external reliability.

Internal reliability checks if an independent researcher
reanalyzing the data would come to the same conclusion. 
To assist other researchers exploring this point, we offer a full replication package for this study at  
https://github.com/ai-se/XTREE\_IST.

External reliability assesses how well independent researchers
could reproduce the study. To increase external
reliability, this paper has taken care to clearly define our
algorithms. Also, all the data used in this work is available
online. 

For the researcher wishing to reproduce our work to other kinds of goals, we offer the following advice:

\bi
\item Find a data source for the other measures of interest;
\item Implement another secondary verification oracle that can assess maintainability, reliability, security, etc;
\item Implement a better primary verification oracle that can do ``better'' than XTREE at finding changes (where ``better'' is defined in terms 
of the opinions of the verification oracle).
\ei


\subsection{Validity}

This paper is a case study that studied the effects of  limiting unnecessary code reorganization on some data sets. This section discusses limitations of such case studies. In this context, validity refers to the extent to which a piece of research actually
investigates what the researcher purports to investigate.
Validity has at least two components: internal and
external validity.

% \subsection{Internal Validity}
% The results of this paper are also specific to the data sets explored here.
% Such sampling bias threatens any data mining case-study; i.e., what matters
% {\em there} may not be true {\em here}. For example, the data sets used here comes from a survey of
% open source JAVA projects from Jureczko et al.~\cite{jureczko10}. Any biases in their selection procedures
% threaten the validity of these results. 

% That said,
% the best we can do is define our methods and publicize our data and code so that other researchers can
% try to repeat our results and, perhaps, point out a previously unknown bias
% in our analysis. Hopefully, other researchers will emulate our methods in
% order to repeat, refute, or improve our results. 

% The recommendation to management, shown on page one of this paper,  was to stop developers doing ``X'' if there was no historical evidence that ``X'' was useful. 
% This recommendation assumes that such an  historical record can be accessed. If not,  then the results of this paper could be use used as a guide (see our Table 8). Alternatively, some form of transfer learning~\cite{Nam15,Jing15,krishna16} could be used to import data from another project (but more research would be required to test the validity of combing tools like XTREE with transfer learning). 

% As with any empirical study, several issues can affect the final results. Therefore, any
% conclusions made from this work must be offered with the following
% caveats.
		
% \subsection{External Validity}
% This section assesses the external validity as favoured by Bosu and Carver~\cite{bosu13}.

Based on the case study presented above,
as well as the discussion in \tion{prelim},
we believe that bad smell indicators (e.g. \mbox{{\em loc}$>$ 100})
have limited external validity beyond the projects from which they are derived. 
While specific models are externally valid,
there may still be general methods like XTREE for finding the good local models.  

Our definition of bad smells is limited to those represented by OO code metrics (a premise often used in related work).   
XTREE, Shatnawi, Alves et al. can  only comment
on bad smells   expressed as code metrics 
in the historical log of a project. 

If developers want to justify their code reorganizations
via bad smells expressed in other terminology,
then the  analysis of this paper must:
\begin{itemize}
	\item Either wait till 
	data about those new
	terms has been collected. 
	\item Or, apply cutting edge transfer learning
	methods~\cite{Nam15,Jing15, krishna16} to map data from other projects
	into the current one.
	\end{itemize}
	Note that the transfer learning approach would
	be highly experimental and require more study
	before it can be safely recommended.
	
	Sampling bias threatens any data mining analysis; i.e., what matters
	there may not be true here. For example, the data sets used here comes from Jureczko et al. and any biases in their selection procedures
	threaten the validity of these results. 
	That said,
	the best we can do is define our methods and publicize our data and code so that other researchers can
	try to repeat our results and, perhaps, point out a previously unknown bias
	in our analysis. Hopefully, other researchers will emulate our methods in
	order to repeat, refute, or improve our results. 

		
		\section{Conclusions}
		How to discourage useless code reorganizations?
		We say: ignore those not supported by the historical log of data from
		the current project.  
		When that data is not available (e.g. early
		in the project) then developers could use the general list of
		bad smells shown in \fig{smells}. However, 
		our results
		show that  bad smell detectors are most
		effective when they are based
		on a small handful of code metrics (as done by XTREE).
		Hence, using all the bad smells of \fig{smells} may not be optimal. 
		
		For our better guess at how to reduce defects by changing code attributes,
		see  Figure ~\ref{fig:multcomp}. But given the large variance if the change recommendations, we strongly advice teams to use
		XTREE on their data to find their own best local changes.
		
		XTREE improves on prior methods for generating bad smells:
		\begin{itemize}
			\item As described in \tion{prelim}, bad smells generated by humans may not be applicable to the current project. On the other hand, XTREE can automatically learn specific thresholds
			for bad smells for the current project.
			\item Prior methods used an old quality predictor (QMOOD) which we replace with defect predictors learned via Random Forests
			from   current project data.
			
			
			\item XTREE's conjunctions proved to be arguably as effective
			as those of Alves (see \fig{jur}) but far less verbose (see \fig{counts}). Since XTREE {\em approves} of fewer changes
			it hence {\em disapproves} of most changes. This makes it a better tool for critiquing and rejecting
			many of the code reorganizations.
		\end{itemize}
		Finally, XTREE does not suffer from the conjunctive fallacy.
		Older methods, such as those proposed by  Shatnawi and Alves assumed
		that the best way to improve code is to remove outlier  values. This may not work since when code is reorganized,
		{\em the functionality has to go somewhere}. Hence,  reducing the lines of code in one module necessitates   
		{\em increasing} the coupling that module
		to other parts of the code. In future, we recommend software team use bad smell detectors that know what attribute measures
		need {\em decreasing} as well as {\em increasing}.
		
		% In summary, this paper offers a baseline result
		% that XTREE can be used as a {\em diagnostic tool}
		% for identifying which bad smells can be ignored.
		% Can it be used as a {\em prescriptive tool}
		% to recommend new refactorings? This would require
		
		% As discussed in {\em PROBLEM \#2}
		% (from the introduction), XTREE might propose
		% changes to code metrics that are
		% not feasible  (due to the local domain constraints
		% of the code). To repair this, XTREE needs to be extended
		% with architectural knowledge of the code it is studying.
		% Elsewhere, we have had some   success
		% with reasoning about such architectural knowledge
		% (expressed as software product lines~\cite{sayyad13a,sayyad13b}). In future
		% work, we will explore extending XTREE 
		% with  
		% architectural knowledge such that it can
		% become a tool which prescribes
		% feasible refactorings. 
		
		
		\section*{Acknowledgements}
		The work is partially funded by NSF  awards \#1506586 and \#1302169.
		
		% \bibliographystyle{plain}
		\balance
		\bibliographystyle{unsrt}
		\bibliography{References}
	\end{document}