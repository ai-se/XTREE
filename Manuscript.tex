\documentclass[twocolumn,5p]{elsarticle}
\usepackage{tikz}
  \def\firstcircle{(90:1.75cm) circle (2.5cm)}
  \def\secondcircle{(210:1.75cm) circle (2.5cm)}
  \def\thirdcircle{(330:1.75cm) circle (2.5cm)} 
\usepackage{comment}
\usepackage{framed,graphicx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath}
\usepackage{bigstrut}
\usepackage{color}
\usepackage{graphics} 
\usepackage{eqparbox}
\usepackage{graphics}
\usepackage{colortbl} 
%\usepackage{times}
 \usepackage{mathptmx} \usepackage[scaled=.90]{helvet} \usepackage{courier}
\usepackage{balance}
\usepackage{picture}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[export]{adjustbox}
\renewcommand{\footnotesize}{\scriptsize}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{lavenderpink}{rgb}{0.98, 0.68, 0.82}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}

\newcommand{\quart}[4]{\begin{picture}(80,4)%1
{\color{black}\put(#3,2){\circle*{4}}\put(#1,2){\line(1,0){#2}}}\end{picture}}

\definecolor{Gray}{gray}{0.95}
\definecolor{LightGray}{gray}{0.975}

\newcommand{\wei}[1]{\textcolor{red}{Wei: #1}} 
\newcommand{\Menzies}[1]{\textcolor{red}{Dr.Menzies: #1}} 

%% timm tricks
\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}


\usepackage[shortlabels]{enumitem}  
\usepackage{url}

\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\usepackage[svgnames]{xcolor}
\usepackage[framed]{ntheorem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shadows}
%\newtheorem{Lesson}{Lesson}
\theoremclass{Lesson}
\theoremstyle{break}

% inner sep=10pt,
\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black,
  fill=Gray!40,  drop shadow={fill=black, opacity=1}]
\newcommand\thmbox[1]{%
  \noindent\begin{tikzpicture}%
  \node [thmbox] (box){%
\begin{minipage}{.94\textwidth}%    
      \vspace{-2mm}#1\vspace{-3mm}%
    \end{minipage}%
  };%
  \end{tikzpicture}}

\let\theoremframecommand\thmbox
\newshadedtheorem{lesson}{Result}
 
 

\begin{document}

\begin{frontmatter}

\title{How to Stop Useless Refactorings}

\author{Rahul Krishna\corref{cor1}\textsuperscript{a,}}
\ead{rkrish11@ncsu.edu}
\author{Tim Menzies\corref{cor1}\textsuperscript{a,}}
\ead{tim.menzies@gmail.com}
\author{Lucas Layman\textsuperscript{b}}
\ead{ llayman@cese.fraunhofer.org }
\cortext[cor1]{Corresponding author: Tel:+1-919-396-4143(Rahul)}
\address{\textsuperscript{a}Department of Computer Science, North Carolina State University, Raleigh, NC, USA\\
\textsuperscript{b}Fraunhofer CESE, College Park, USA}
\pagestyle{arabic}

\begin{abstract} 
{\bf Context: } 
Developers use code bad smells to gudie their refactoring (reorganizing their code).
Yet developers, text books, and tools disagree on which bad smells are important.
%Many bad smell detectors recommend refactoring when static code measure
%cross some threshold boundary. 
%%However, it is unclear at what threshold values smell-related metrics
%should trigger code improvement or preventative maintenance.  

\noindent 
{\bf Objective: } We seek to a tool that can recognize  useless refactorings.

\noindent
{\bf Method: } We introduce XTREE, a tool which, given a historical log of defects seen previously in the code, generates
a set of useful code changes.
Any bad smell that requires changes outside of that set can be deprioritized 
(since there is no
historical evidence that that bad smell causes any problems).

\noindent
{\bf Evaluation: } We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts.  

\noindent
{\bf Results: }Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations in previous studies. Further, XTREE endorses changes to very few code metrics, and those bad smell recommendations (learned from previous studies) are not universal to all  software projects.

\noindent
{\bf Conclusion: }
Before undertaking a refactoring based on a bad smell report,
use a tool like XTREE to check and ignore any refactoring   that  are useless; i.e. which lacks evidence
  in the historical record  that it is useful to make that change.
  Note that this use case applies to both manual refactorings proposed by developers
as well as refactoring  conducted by automatic methods.
This recommendation assumes that there is an historical record.
If none exists, then the results of this paper could be use used as a guide (see our
Table~8). 


\end{abstract}
\end{frontmatter}
\pagenumbering{arabic} %XXX delete before submission

\vspace{1mm}
\noindent
{\bf Keywords:} Bad smells,
performance prediction,  decision trees 




\section{Introduction}

\noindent
%%``Managing people is like herding cats'', says management consultant Watler
%Bennis~\cite{bennis97}. Nowhere is this more true than when managing programmers. 
%Programmers can be very intelligent, but very opinionated people whose
%social position within software organizations relies, in part,
%of perceived notions of technical excellence. Hence, it can
%be hard for managers to suggest to programmers that are
%wrong about how they are prioritizing their development effort.

%Consider, for example, the activity of refactoring code based on bad smells.
According to   Fowler ~\cite{fowler99}, bad smells (a.k.a. code smells)
are ``a surface indication that usually corresponds to a deeper problem''.
Fowler strongly recommends   removing   code smells   by
\begin{quote}
``...applying a series of small behavior-preserving transformations, each 
of which seem ``too small to be worth doing''. 
The  effect of   these \underline{refactoring} transformations is quite significant. By doing them in small steps you reduce the risk 
of introducing errors''.
\end{quote}
Bad smells are a widely used technique for evaluating code. As of  early 2016, ACM Digital Library reports 
the Fowler text of {\em Refactoring} has 6434 references~\cite{fowler99}. A search  on {\em ``bad smell''programming} 
where {\em proceedings=ICSE} finds 1,748 papers since 2006.
Hence it is important to understand the value of
this popular topic.  
Valuable development resources may be wasted on bad smells that are ``bad''; i.e. 
 irrelevant or misleading
for some particular project.

In our experience, programmers can use the SE literature to justify extensive, perhaps even excessive, refactoring efforts.
For example, this paper began when a Washington-based software company shared
one of their management challenges with us: their  releases are delayed by developers spending  much time removing bad smells within their code. This management team wanted some way to check if all that activity was useful.
 
Note that these managers are {\em not} asking for better
automatic refactoring
tool support. Their human programmers are refactoring
code all the time, perhaps too much so.
Hence, their management goal is: 
\bi
\item
{\em Not} to optimize {\em necessary} refactorings;
\item
But rather to stop {\em unnecessary} refactorings.
\ei
For those managers, we offer the  following heuristic:
\bi
\item If there is no evidence in the historical record that changing ``X'' is useful;
\item Then stop developers from doing ``X''.
\ei
This paper describes one way to operationalize this heuristic, in the context
of assessing bad smells. We use data mining to generate two oracles: a {\em primary
change oracle} and a {\em secondary validation oracle}.
By combining these two oracles,
 we can generate and validate useful
changes to code modules.

In this paper, we use 
the XTREE cluster delta algorithm as the {\em primary change  oracle} .
XTREE 
explores the historical record of a project to find clusters of projects.
It then proposes a ``minimal'' set of changes $\Delta$ that can move a software module $M$ from a defective cluster $C_0$ to another with fewer defects$C_1$ (so $\Delta$
is some subset of $C_1 - C_0$). To define ``minimal'', XTREE uses an entropy measure.

As to 
the {\em secondary validation oracle}, this  checks if the primary oracle is proposing
sensible changes. For this purpose we  use Random Forests\cite{XXX}, augmented with SMOTE (synthetic  minority over-sampling technique~\cite{YYY}).
 In our framework,  learning
the secondary oracle is   a {\em separate} task to learning the primary
oracle. This  ensures that the validation oracle offers an independent
opinion on the value of the proposed changes.

When these oracles were applied to defect data from
open source object-oriented software systems, we discovered a {\em conjunctive fallacy}
in the bad smell literature~\cite{erni96,bender99,Shatnawi10,Alves2010,hermans15}.
A common heuristic is to recommend refactoring of ``bad'' code
to make it ``better''   as follows: for all static code measures that exceed some threshold, make changes such that the thresholds are no longer exceeded. That is:
\begin{equation}\label{eq:df}
\scriptstyle
\begin{array}{rl}
\mathit{bad} & = \left(a_1 > t_1 \right) \bigvee \left(a_2 > t_2\right) \bigvee    ... \\
\mathit{better} & = \neg\;\mathit{bad} = \left(a_1 \le t_1 \right) \bigwedge \left(a_2 \le t_2\right)  \bigwedge  ...
\end{array}
\end{equation}
We say that the above definition of ``better'' is a conjunctive fallacy
since it assumes that the best way to improve code is to decreases multiple code attribute measures below $t_i$ in order
to remove the ``bad'' smells. This is wrong since 
there exists an intricate set of associations between static code measures
such that {\em decreasing}  $a_i$ necessitates {\em increasing} $a_j$.
It is easy to see why this is so.
A requirement for  refactoring is that the new code supports the same functionality
as before. If we pull code out of a function (since that function has grown too
large), {\em that functionality has to go somewhere else}. What we see in XTREE's change
recommendation is that if we decrease a module's lines of code  then in the refactored
code, the functionality is achieved via a communication between several different parts
of the system. That is, {\em decreasing} the lines of code metric within a module {\em increases}
that module's coupling metrics.


 
  
  
  The rest of this paper is structured as follows. First, we offer some background on bad smells. After that,
  in Section~3, 
  different methods for determining when code does/does not have bad smell. One of those methods
  will be XTREE while the others will come from other researchers. Next,    Section~4
  defines an assessment method that checks which of these methods is most useful. The results from that assessment 
  are presented in Section~5. Those results will answer five research questions.

 {\bf  RQ1: Effectiveness}: According to the validation oracle, which of the methods  defined in Section~3 is the best  change oracle for identifying what and how
  code modules should be changed? To answer this question, we used data from five OO Java projects
  (Ivy, Lucene, Ant, Poi, Jedit). It was found that:
  \begin{lesson}
 XTREE is the best oracle on how to change code modules (in order to reduce defects).
  \end{lesson}

 {\bf RQ2: Succinctness}: Our goal is to critique and, possibly,
ignore irrelevant bad smell detectors.  If those changes are minimal (i.e. effect fewest attributes) then those changes
will be easiest to apply and monitor. Hence, we ask, which of the Section~3 methods recommended changes to the fewest
code attributes?
\begin{lesson}
Of all the code change oracles studied here, XTREE recommend  far fewest changes to static code measures.
\end{lesson}

 {\bf RQ3: Stopping}: Our advice to managers is to stop refactoring based on changes that lack
historical evidence of being effective. How effective is XTREE at offering such ``stopping points'' (i.e. clear guidance on what {\em not} to do)??
\begin{lesson}
In  any  project,  XTREE’s  recommended  changes  only  mentions one to four 
of the  static code attributes.  Any bad smell defined in terms of the remaining 19 to 16 code attributes (i.e. most of them)
would hence be deprecated.
\end{lesson}

 {\bf RQ4: Stability}: Across different projects, how variable are the changes recommended by our best change oracle?
\begin{lesson}
The direction of change recommended  XTREE is  very stable across repeated runs of the program 
\end{lesson} 

{\bf RQ5: Conjunctive Fallacy}:  
Is it always  useful  to aplly \eq{df}; i.e. make code better by   reducing the values of  multiple code attributes? We find that:
\begin{lesson}
XTREE usually recommends reducing lines of code (size of the modules).
That said,  XTREE often recommends {\em increasing} the values of other static code attributes.
\end{lesson} 
Note that {\bf RQ3, RQ4, RQ5} 
confirms the intuitions
of the project managers that prompted this investigation:
\bi
\item
Yes,  indeed, their programmers
were wasting time on   performing refactorings  that  decreased multiple measures when, in fact,
they should be trying to {\em decrease} some measures while {\em increasing} others.
\item
Across all studied projects, XTREE found that  around one-fifth  
  of the measures were usually found within  change recommendations. That is, it is 80\% likely that
  that refactoring based on reducing any one measurement picked at random will be useful.
\ei
Consequently, we  recommend that XTREE be used as follows:
\bi
\item Before doing  refactoring based on a bad smell report...
\item ...check and stop any refactoring   for which there is no proof
  in the historical record that that change improves the code.
\ei
Note that this use case applies to both manual refactorings proposed by developers
as well as refactoring as conducted by automatic methods~\cite{mkaouer2015many}.
Note also that
this recommendation assumes that there is an historical record. If none exists, then the results of this paper could be use used as a guide (see our Table 8).
 
 \subsection{Replication Package}
 
  
  In order to support dissemination
  of research results, and to encourage other researchers to repeat our results,
  all the code used in this paper is publicly available\footnote{XXX: need a public repo}.
  
  \subsection{Relationship to Prior Work }
A four page prelininary report on the XTREE system~\cite{krishna2015actionable} has been presented previously\footnote{https://goo.gl/2In3Lr}. That short report offered case studies only two  of
the five data sets studied here. Also, that prior work had:
\bi
\item
no comparison to any baseline
method from other researchers, whereas this paper compares XTREE's recommendations
with those found by researchers exploring bad smells.
\item
no {\em secondary validation oracle}, i.e. no way to check if XTREE's recommended changes were sensible.
\ei
Further, of this entire paper, the only sections containing material
found in prior papers is 3.2.1, 3.2.2 as well as two-fifths of \fig{}.
%YYY section1,2, 3.1,4,
% Prior to XTREE, the authors worked on
% the CD algorithm discussed in ~\tion{cdcd}. 
% The goal of CD was the same as WHERE-- find recommendations on how to best change software
% modules. However, as discussed in~\tion{cdcd},
% CD worked very different to WHERE. Further, as shown in this paper,
% CD is significantly out-performed by XTREE.



\section{Why Not Just Ask Developers to Rank Bad Smells?}\label{sect:prelim}

Why build tools like XTREE to  critique proposed developer actions?
Our answer, documented in this section, is that (1)~SE literature is not clear which bad smells are ignorable or  high priority and must be resolved.
Also, (2)~developers themselves are unclear on what bad smells are most important to fix.
 
 
Much research endorses them to guide
code improvement (e.g., refactoring or preventative maintenance). A recent literature review by Tufano et al.~\cite{Tufano2015}  
lists dozens of papers on smell detection and repair tools. 
Yet
other papers cast doubt on the value of bad smells
as triggers for code improvement~\cite{Mantyla2004,Yamashita2013,Sjoberg2013}. 
As we show in this paper,  
 tools, text books, and developers disagree on what bad smells
are important enough to hunt down and eliminate. Our findings
are consistent with other research suggesting that universal bad
smells that ignore project context are not useful to guide refactoring~\cite{Mantyla2004,Yamashita2013,Sjoberg2013}.

If the SE literature is contradictory, why not ignore it and use domain experts (software engineers) to decide
what bad smells to fix? We do not recommend this since developer {\em cognitive biases} can mislead them to
assert that some things are important and relevant when they are not. 
A widespread malaise in software engineering is that
beliefs about software are rarely revised and hence may be
  inaccurate and 
misleading~\cite{passos11,jorgensen09,mei15,me16phase,prem16}. Software developers may have strong views on many issues, including bad smells, but those views may be wrong for the current
project.
Software engineering is not the only field with this problem.
  For example, the medical profession applies many practices based on studies that have been disproved (a recent article in the Mayo Clinic Proceedings~\cite{prasad13} found 146 medical practices based on studies in year i, but which were reversed by subsequent trials within years i + 10). Even when the evidence for or against a treatment or intervention is clear, medical providers and patients may not accept it~\cite{aschwanden10}. Aschwanden warns that {\em cognitive biases} such as confirmation bias (the tendency to look for evidence that supports what you already know and to ignore the rest) influence how we process information~\cite{aschwanden15}.

As in medicine, so too in software engineering.
According to Passos et al.~\cite{passos11},  developers often  assume that the lessons they learn from a few past projects are general to all their future projects. They comment ``past experiences were taken into account without much consideration for their context''~\cite{passos11}.  J{\o}rgensen \& Gruschke~\cite{jorgensen09} offer a similar warning. They report that the supposed software engineering ``gurus'' rarely use lessons from past projects to improve their future reasoning and that such poor past advice can be detrimental to new projects.~\cite{jorgensen09}. Other studies have shown some widely-held views are   now questionable given new evidence:

\bi
% COMMENT FROM LL: This bullet is weak. Maybe goto isn't used in an unrestricted manner because of Djikstra's warning.
% \item
% Nagappan et al. analyzed~\cite{mei15} 
% how   {\tt goto}  was used  in 11,000 Github repositories. 
% They found that developers
% rarely used {\tt goto} in the unrestricted manner that alarmed Dijkstra in his  1968
% memo ``Go to statement considered harmful''~\cite{Dijkstra68}.
\item
In other work~\cite{me16phase} with the Software Engineering Institute (SEI), we have revisited
the ``phase delay'' truism that {\em the cost of fixing an error dramatically increases the longer it is in the system}. 
We found no evidence for such large   phase delay effect in modern
software (in 171 software projects shepherded
by the SEI, 2006-2014) possibly since it has been  heavily mitigated by $21^{st}$ century software development languages, tools and development practices. 
Whatever the reason, the main point  is that phase delay is a widely
held belief, despite little recent evidence to support it.
\item
Devanbu et al.  examined responses from 564 Microsoft software developers from around
the world, they found that  ``(a)~programmers do indeed have very
strong beliefs on certain topics; (b)~their beliefs are primarily formed
based on personal experience, rather than on findings in empirical
research; (c)~beliefs can vary with each project, but do not necessarily
correspond with actual evidence in that project''~\cite{prem16}.
\ei
Devanbu et al. further  comment that ``programmers give personal experience
as the strongest influence in forming their opinions''. This is a troubling
result, especially given the above comments from Passos and  J{\o}rgensen et al.~\cite{passos11,jorgensen09} about how quickly practitioners form, freeze, and rarely revist those opinions.



\input{tex/refactor}

If the above remarks hold true for bad smells, then we would expect
to see much disagreement on which bad smells are important and relevant
to  a particular project. This is indeed the case.
The first column of \fig{smells} 
lists  commonly mentioned bad smells and comes from Fowler's 1999 text~\cite{fowler99} and a subsequent 2005 text by Kerievsky that is widely cited~\cite{Kerievsky2005}.
The other
columns show data from other studies on which bad smells matter most.
The columns marked as Lanza'06 and Yamashita'13 are from peer reviewed literature. The column maked SonarQube is a popular open source
code assessment tool that includes detectors for six of the bad smells
in column one. 
The {\em developer survey}   (in the right-hand-side column),
  this shows the results of an hour-long whiteboard sessions with a group of 12 developers from a Washington
    D.C. web tools development company. Participants
    worked in a round robin manner to rank the bad smells they thought were
    important (and any disagreements were discussed with the whole group).
     Amongst the group, there was  some
    consensus on  the priority of which bad smells to fix
    (see the annotations VH=very high,
    H=high, L=low, VL=very low, and ``?''= no consensus).  
   
   
   
\input{tex/jur_char}

A  blank cell in \fig{smells}
indicates where   other work has chosen to ignore
one the   bad smells in column one. 
Note that most of the cells are blank, and that the studies ignore the majority of the Fowler bad smells.
SonarQube has no detectors for many of the column one bad smells.
Also, nearly half the Yamashita list of bad smells
    does not appear in the Fowler and Kerievsky list. The eight numbers
    in the  Yamashita'13 column show the rankings for the bad smells 
    that overlap with Fowler and Kerievsky; Yamashita also discussed other smells not covered in Fowler'99.
    

Two of the studies in \fig{smells} offers some comments on the relative importance
of the different bad smells in the Yamashita'13 study and the developer study. Three of the bad smells listed in the top half of the Yamashita'13 rankings also score very high in the developer survey. Those three were {\em duplicated code, large class}, 
        and {\em long method}. 
 Note that this agreement also means that the
  Yamashita'13 study and the developer survey   
  believe
        that very few  code smells are   high priority issues
        requiring refactoring. 
        
In summary, just because one developer strongly believes in the importance of a bad smells does not mean that belief transfers to other developers or projects.
Developers can be clever, but their thinking can also be distorted
by cognitive biases.
Hence, as shown in \fig{smells}, developers, text books, and tools 
can disagree on which bad smells are important.
Special tools are needed to assess their beliefs, for example, their beliefs in
bad smells.  
 

\section{Learning Bad Smell Thresholds}\label{sect:bst}

Having made the case for automatic support for assessing bad smells,
this section reviews different ways for building those tools (one of those tools,
XTREE, will be our recommended {\em primary change oracle}).
Later in this paper, we will offer  a {\em secondary validation oracle}
that checks the effectiveness of the changes proposed by XTREE.

The SE literature offers  two ways to 
learning bad smell thresholds.
One approach relies on 
{\em outlier statistics}~\cite{erni96,bender99}. This approach
has been used   by Shatnawi~\cite{Shatnawi10}, Alvenes et al.~\cite{Alves2010}
    and Hermans et al.~\cite{hermans15}.
    Another approach is 
    based on {\em cluster deltas} that we developed
    for   Centroid Deltas~\cite{me12c} and 
    use here for XTREE. 
    These two approaches are discussed below. 

The outlier approach assume that unusually large measurements indicate risk-prone code.
Hence, they generate one bad smell threshold for any metric
with such an ``unusually large'' measurements. 
The literature lists several ways to define ``unusually large''.

\subsection{Outlier Statistics}

\subsubsection{Enri \& Lewerentz}
Given classes described with the  code metrics of \fig{ck},
Enri and Lewerentzz~\cite{erni96} found the   mean $\mu$ and the standard deviation $\sigma$
of each
code metrics. Their definition of problematic outlier was any code
metric with a measurement greater than $\mu+\sigma$.
Shatnawi and Alves et al.~\cite{Shatnawi10,Alves2010} depreciate
using $\mu+\sigma$ since it does not consider the fault-proneness of classes when the thresholds are computed. Also, the method lacks  empirical validation.

\subsubsection{ Shatnawi}
Shatnawi~\cite{Shatnawi10}'s preferred alternative to $\mu+\sigma$
is to use the VARL method (Value of Acceptable Risk Level) proposed by Bender~\cite{bender99} in his epidemiology studies.  This approach uses two
thresholds which, following Shatnawi's guidance, we set to
$p_0=p_1=0.05$. 

VARL encodes the defect count
for each class as 0 (no defects known in class) or 1 (defects known in class).
Univariate binary logistic regression is applied to learn three coefficients:  
 $\alpha$ is the intercept constant;
    $\beta$ is the coefficient for maximizing log-likelihood;
  and $p_0$  
    measures   how well this   model predicts for   defects.
Any code metric with $p_0>0.05$ is  ignored as being a poor defect predictor. Thresholds are then learned from the surviving metrics $M_c$ using
an equation proposed by Bender:
\begin{equation}
 \mathit{bad\; smell\; if\;} M_c > \frac{1}{\beta }\left( {\log \left( {\frac{{{p_1}}}{{1 - {p_1}}}} \right) - \alpha } \right) 
\end{equation}

 
\subsubsection{ Alves et al.}
Alves et al.~\cite{Alves2010} propose another approach
to finding thresholds that  utilizes the underlying statistical distribution and scale of the metrics. 
Metric values for each class are weighted according to the source lines of code (SLOC) of the class. All the weighted metrics are then normalized by the sum of all weights of for the system. 
The normalized metric values are ordered in an ascending fashion (this is equivalent to computing a density function, in which the x-axis represents the weight ratio (0-100\%), and the y-axis the metric scale).
Alves et al. then select a percentage value (they suggest 90\%) which represents the ``normal'' values for metrics. The metric threshold, then, is the metric value for which 90\% of the classes fall below. The intuition  is that the worst code has outliers beyond 90\% of the normal code measurements. Hermans et al.~\cite{hermans15} used the
Alves et al. method in their  2015 paper on
exploring bad smells.

For consistency
with Shatnawi, we explore the correlation between
the code metrics and the defect counts and   reject code metrics that are poor predictors of defects (i.e.   those  with $p > 0.05$).


\begin{figure*}[!t]
~\hrule~
\begin{minipage}{.53\linewidth}
\small
 \begin{tabular}{p{0.95\linewidth}} 
 On the right-hand-side is a tree
 generated by iterative dichomization. 
 This tree can be read like a nested if-then-else statement; e.g.
 \begin{itemize}
     \item Lines 3 and 8 show two branches for lines of code (denoted here as {\em `\$loc}) below 698 and above 698.
     \item Any line with a colon ":" character shows  a  leaf  of  this  nesting.   For  example,  if  some  new  code module is passed down this tree and falls to the line marked in \textcolor{orange}{{\bf orange}},  the colon on that line indicates a prediction that this module has a 100\% chance of being defective. 
     \end{itemize}
Using this tree,
 XTREE looks for a nearby branch that has a lower chance of being defective. Finding the \textcolor{green}{{ green}} desired  branch,  XTREE reports a bad smell threshold for that module that is the delta between the \textcolor{orange}{{\bf orange}}  current branch  and   \textcolor{green}{{ green}} designed branch. 
 In this case, that threshold relates to:
 \begin{itemize}
     \item Lines of code and comments ({\em lcom}) 
     \item The cohesion between classes ({\em cam}) which measures similarity of parameter lists to assess the relatedness amongst class   methods.
     \end{itemize}\\ 
 \end{tabular}
 \end{minipage}~~~~
 \begin{minipage}{.45\linewidth}
\includegraphics[width=\linewidth]{figs/XTREE_samp.png}
\end{minipage}
~\hrule~
 \caption{A brief tutorial on XTREE.} \label{fig:xtree_samp}
\end{figure*}

\subsubsection{Discussion of Outlier Methods}\label{sect:disc}
The advantage of the outlier-based
approaches is that they are simple to implement, but the approaches have   two  major disadvantages. 
First, they are {\em verbose}. A threshold can be calculated for every metric -- so, which one should the developers focus on changing? Without a means for prioritizing the  thresholds and metrics against one another, developers may have numerous or conflicting recommendations on what to improve. Second, the outlier approaches suffers from
the {\em disjunction fallacy}  discussed in the introduction.
That is, while
 they propose thresholds for many code metrics
individually, they make no comment on what minimal
metrics need to be changed at the same time (or whether or not those
changes need to minimizations or maximations of static code measures).



\subsection{Cluster Deltas}

Cluster deltas are a general method
for learning {\em conjunctions} of changes
that need to be applied at the same time. 
This approach works as follows:
\begin{itemize}
    \item Cluster the data. 
    \item Find
neighboring clusters $C_+,C_-$ where $C_+$ has more examples of defective
modules than $C_-$;
\item Compute the  delta   in code metrics between the clusters using \mbox{$\Delta = C_+ - C_-$}, i.e.
{\em towards} the cluster with lower defects;
\item $\Delta$ are changes needed in defective modules of $C_+$ to
      make them more like the less-defective modules of $C_-$
\end{itemize}
Note that $\Delta$ is a conjunction of  recommendations.
Since it is computed
from neighboring clusters, the examples contain similar distributions and $\Delta$ respects the naturally occurring constraints in the data. For example,
given a bad smell pertaining to large methods,   $\Delta$   will not  suggest lowering lines of code
without also increasing a coupling measure. 
Cluster deltas are used in CD~\cite{me12c} and XTREE.

\subsubsection{CD}\label{sec:cdcd}
Borges and Menzies first proposed the CD centroid delta approach to
generate {\em conjunctions} of code metrics
that need to be changes at the same time
in order to reduce defects~\cite{me12c}.
CD used the WHERE clustering algorithm developed by the
authors for a prior application~\cite{localvsglobal}.
Each cluster was then replaced by its centroid
and $\Delta$ was calculated directly from the difference
between code metrics values between one centroid
and its nearest neighbor.


As shown below, one drawback with with CD is that it is {\em verbose}
since
CD   recommended changes to all code
metrics with different values in those two centroids. 
This makes it hard to use CD to   critique and prune away bad smells. Further, CD will be shown to be
not as effective
in proposing changes to reduce defects as XTREE.

\subsubsection{XTREE: Overview}

  XTREE  is a cluster delta algorithm
  that avoids the problem of verbose $\Delta$s.
  It is our recommended {\em primary change oracle} that makes recommendations 
  of what changes should be made to code modules.
  Instead of reasoning over cluster centroids,
  XTREE utilizes a decision tree learning approach
  to find the fewest differences between clusters of examples.
  
  
 XTREE uses an iterative dichotomizer that
first converts the values for each code
metrics into a small number of ranges.
Consider a code metric is split into $r$ ranges and each range is of
  size
    $n_r$ is
  associated with a set of defect counts $x$ with standard deviation
  $\sigma_r$.
  The best split for that range is
  the one that minimizes the expected value of the
  defect variance, after the split; i.e.
  $\sum_r\frac{n_r}{n}\sigma_x$ (where $n=\sum_r n_r$).
  This discretizer then recurses on each part of the split
  to find other splits. 
  
  When discretization finishes, each code metric $M$ has a 
  final expected value $M_v$ for the defect standard deviation 
  across all the discretized ranges of that metric.
  Iterative dichomization sorts the metrics by $M_c$
  to find the best ``splitter'' for the data
  (the code metric with smallest $M_v$). The data
  is then split on the ranges of the splitter and
  iterative dichomization recurses on each range.  In this way,
  each split is the root of a sub-tree.
  

 
  While not usually labelled as such, iterative dichomization can
  be viewed as a clustering algorithm that groups together code modules
  with similar defect counts and some shared ranges of code metrics.
  For our purposes, we score each cluster found in this way according
  to the percent of classes with known defects. For example,
  the last line of \fig{xtree_samp} shows a tree lead with 75\%
  defective modules.
  
 \fig{xtree_samp} offers
  a small example of how XTREE builds
    $\Delta$ by comparing branches that lead to leaf clusters
  with different defect percentages. In this example, assume a project with a table of code metrics data describing its classes in the form of \fig{ck}. After code inspections and running test cases or operational
      tests, each such class is augmented with a defect count.
      Iterative dichomization takes that table of data and, 
        generates the tree of \fig{xtree_samp}.
        
     Once the tree is built, a class with code metric data is passed into the tree and evaluated down the tree to a leaf node (see the \textcolor{orange}{{\bf orange}} line in \fig{xtree_samp}).
      XTREE then looks for a nearby leaf node with a lower defect
      count (see the \textcolor{green}{{green}} line in \fig{xtree_samp}). For that evaluated class, XTREE proposes a bad smell
      thresholds that is  the difference between 
      \textcolor{green}{{green}} and \textcolor{orange}{{\bf orange}}. 
  
  
\subsubsection{XTREE:   Details}

Using the training data,  XTREE uses  
iterative dichomization to
  divide  $N$ code modules  into  clusters of
size $\sqrt{N}$.

Next, for each test code module, 
	  find $C_+$ as follows take each test , run it down to a leaf in the decision tree.  
After that,	  find $C_-$ as follows:
		\begin{itemize}
		\item Starting at the  $C_+$ leaf, ascend   $lvl\in \{0,1,2...\}$ tree levels;
		\item Identify {\em sibling} leaves; i.e. leaf clusters that can be reached from level $lvl$ that are not same as {\em current }
		\item Find the {\em better} siblings; i.e. those 50\% (or less)
		fewer defects than $C_+$.
		   If none found, then repeat for $lvl += 1$. Also,
		    return nil  if the new $lvl$ is above the root. 
		\item  Set $C_-$ to the  {\em closest} better sibling where distance is measured between the mean centroids of that sibling and {\em current}
		\ei
	 Now find $\Delta = C_+ - C_-$  by reflecting
	 on the set difference between  conditions in the decision tree branch to $C_-$ and $C_+$. To find that delta,
	 for discrete attributes, the delta is the value from {\em desired};
	    for numerics  expressed as ranges, the delta is a random number selected from the low and high boundaries of the that range. 
	  

\input{tex/prediction_stats.tex}      
\section{Evaluation}
The previous section proposed numerous methods for detecting bad smells that need
to be resolved. This section offers an experiment of the following form:
\bi
\item Using each method as a {\em primary change oracle} to recommend how code should be changed.
\item Apply those changes. 
\item Run a {\em secondary validation oracle} to assess the defect-proness of the changed code.
\item Sort the change oracles on how well they reduce defects (as judged by the validation oracle).
\ei
Using this experiment, we can address the research questions dicsussed in the introduction.

{\bf RQ1: Effectiveness: } which of the methods defined in Section 3 is the best change oracle for identifying what and how code modules should be changed? 
To   answer to this question, we will assume that developers
refactor their code until the bad smell thresholds is not violated.
This refactoring will start with some {\em initial} code
base that is changed to a {\em new} code base. 
For example, if the bad smell is \mbox{{\em loc $>100$}} and a 
code module has 500 lines of code, we reason
optimistically that we can change that code metric
to 100.  
Using the secondary validation oracle,  we then predict the
number of defects in $d_+,d_-$ in {\em initial} and {\em new}. We evaluate the performance of XTREE, CD, Shatnawi, and Alves methods in setting bad smell thresholds.
The best bad smell threshold method is the one that maximizes
\begin{equation}\label{eq:diff}
\mathit{improvement} = 100* \left(1 - \frac{ d_- }{ d_+}\right)
\end{equation}


{\bf RQ2: Succinctness: } which of the Section 3 methods recommended changes to the fewest code attributes?
To answer this question, we will report the frequency at which different attributes are selected in repeated runs of our oracles.

{\bf RQ3: Stopping: }    How effective is XTREE at offering   ``stopping points'' (i.e. clear guidance on what {\em not} to do)?   
To answer this point, we will report how often XTREE's recommendations {\em omit} a code attribute. 
Note that the {\em more often} XTREE omits an attribute, the more likely it is {\em not} to endorse a bad smell based on the omitted
attribites.

{\bf RQ4: Stability:} Across different projects, how variable are the changes recommended by XTREE?   To answer this
question, we conduct a large scale ``what if'' study that reports all the possible recommendations XTREE might make.
We then count often attributes are {\em not} found in the recommendations arising from this ``what if'' study.

{\bf RQ5: Conjunctive Fallacy:} Is  it  always  useful  to  aplly \eq{df}; i.e.   make  code  better  by  reducing  the  values  of multiple code attributes? To answer this question, we will look at the {\em direction of change} seen in the {\bf RQ4} study; i.e.
how often does XTREE recommend decreasing or increasing a static code attribute.


 
  

\subsection{Test Data}\label{sect:tesd}

To explore these research questions,
we used data from
Jureczko et al.'s collection of object-oriented Java systems~\cite{jureczko10}. To access that data, go to   git.io/vGYxc.
The Jureczko data records the number of known defects for each class using a post-release bug tracking system. The classes are described in terms of nearly two dozen metrics included in the Chidamber and Kemerer metric suite, such as number of children (noc), lines of code (loc), etc. For details on the Jureczko code
metrics, see  \fig{ck}. For details on the rows and versions
of that data, see the left-hand-side columns of \fig{jur}.



 
\subsection{Secondary Validation Oracles}
\label{sect:eval}

As mentioned in the introduction, our proposed framework has two oracles:
a primary change oracle (XTREE) and the secondary validation oracle described in this section.

It can be difficult  to judge the  effects of removing bad smells.
Refactored code cannot be assessed just by a rerun of the the test
suite (since refactorings are supposed to preserve behavior). 
% waiting to observe 
% code refactorings is meant to
% preserve the behavior of the system (i.e. the same tests
% should pass before and after the refactoring). 
Also, it many not be practical
 to assessing refactorings by making the
refactorings,  then seeing what happens to that code for the rest
of the project. In a test-driven agile project, as
SCRUM meetings change what stories are implemented next,
the operational profile that lead to defects in the old
code may not be repeated in future executions. 

To resolve this  problem, SE researchers such as 
Cheng et al.~\cite{Cheng10}, O'Keefe et al.~\cite{OKeeffe08,OKeeffe07},
Moghadam~\cite{Moghadam2011} and Mkaouer et al.~\cite{Mkaouer14}
use a {\em secondary validation oracle} (which is learned separately
to the primary oracle) that   assesses
how defective is the code before and after some
refactoring. 
For their second oracle,
 Cheng, O'Keefe, Moghadam and  Mkaouer et al. use the QMOOD hierarchical
quality model~\cite{Bansiya02}.
A shortcoming of QMOOD
is that quality models learned from other projects
may perform poorly when applied to new projects~\cite{localvsglobal}.
Hence, for this study, we  eschew
older quality models like QMOOD. Instead, we use
Random Forests~\cite{Breiman2001} to learn defect predictors
from OO code metrics.
Note that, unlilke QMOOD, such measurements 
  are specific to the project, and the measurements can be reconstructed and the predictors rebuilt for other projects.
 
  Random Forests are a decision tree learning method but
  instead of building one tree, hundreds are built using
  randomly selected subsets of the data. The final predictions
  come from averaging the predictions over all the trees.
  Recent studies endorsed the use
  of  Random Forests for  defect prediction~\cite{lessmann}.
  
\fig{j} shows   studies with Random Forests and
the Jureczko data. Given   $V$ released versions, we test on version $V$ and train on the available data from $V-1$ earlier releases (as shown in \fig{j}. Note the   \colorbox{lavenderpink}{three bottom rows}   marked with $\times$: these contain predominately defective classes (two-thirds, or more).  In such data, it is hard to distinguish good examples (due to all the bad examples). 

In order to identify the presence (or absence) of defects, we can   use Boolean classes in the  Jureczko data ( \texttt{True} if defects \textgreater 0; \texttt{False} if defects = 0). For such data, the quality of the predictor can be measured using (a) the  probability of detection (a.k.a. ``pd'' or recall):  the percent of faulty classes in the test data detected by the {\em predictor}; and (b) the  probability of false alarm (a.k.a. ``pf''): the percent of non-fault classes that are {\em predicted} to be defective.

The ``untuned'' columns of \fig{j}
show a preliminary study. This study used
Random Forest with its ``off-the-shelf'' tunings (i.e.  
100 trees per forest).  
The forests were built from training data and applied to test data
not seen during training.  In this
study, we called a data set ``usable'' if   Random Forest was able to classify the instances with a performance threshold of $\mathit{pd}\ge 60 \wedge \mathit{pf} \le 30$\% (determined from standard results in other publications~\cite{me07b}). Note that no  data set meet
that criteria.

The ``tuned'' columns of \fig{j} show that we can salvage some of the data sets. We applied both the SMOTE algorithm and differential evolution to improve the performance of the classifier. Pelayo and Dick~\cite{pelayo07} report that defect prediction is improved by SMOTE~\cite{Chawla2002}; i.e. an over-sampling of minority-class examples and an under-sampling of majority-class examples. Fu et al.~\cite{fu:ase15} report that parameter tuning with differential evolution~\cite{storn97} can quickly explore the tuning options of Random Forest to find better settings for the (e.g.) size of the forest, the termination criteria
for tree generation, etc. We note that SMOTE-ing and
parameter tunings were applied to the training data only and not to the test data.

The rows \colorbox{celadon}{marked with a $\star$} in \fig{j} show data sets whose performance was improved remarkably by these techniques. For example, in {\em poi}, the recall increased by 4\% while the false alarm rate dropped by 21\%. However, as might have been expected, we could not salvage the data sets in the  three bottom rows.

We eliminate the data sets for which we could not build an adequately performing Random Forest classifier with $\mathit{pd}\ge 60 \wedge \mathit{pf} \le 30$\%. Thus, our analysis uses the {\em jedit, ivy, ant, lucene} and {\em poi} for evaluating recommended changes.




\subsection{Statistics}


We use 40 repeated runs, each with different random number seeds (we use 40 since that is  more than the 30 samples  needed to satisfy the central limit theorem). Each run collects the \eq{diff} values.
We use multiple runs since two of our methods use some random choices: CD uses the  stochastic WHERE clustering algorithm~\cite{localvsglobal}
while XTREE non-deterministically picks thresholds randomly from
the high and low boundary of a range. 
 Hence, to compare all
four methods, we must run the analysis many times. 



\input{tex/results_jur.tex}

To rank these 40 numbers collected from CD, XTREE, Shatnawi, and Alves et al., we use the Scott-Knott test recommended by Mittas and Angelis~\cite{mittas13}. 
Scott-Knott is a top-down clustering approach used to rank different
treatments. If that clustering finds an interesting division of the data, then
some statistical test is applied to the two divisions to check if they
are statistically significant different. If so, Scott-Knott recurses
into both halves.

To  apply Scott-Knott,
  we
sorted a list of  $l=40$ values of \eq{diff} values found in  $ls=4$ different methods. 
Then, we split $l$ into sub-lists $m,n$ in order to maximize the expected value of differences in the observed performances before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$: \[E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2\]
We then apply a apply a statistical hypothesis test $H$ to check
if $m,n$ are significantly different  (in our case, the conjunction of A12 and bootstrapping). If so, Scott-Knott recurses on the splits. In other words, we divide the data if \textit{both} bootstrap sampling and effect size test agree that a division is statistically significant (with a confidence of 99\%) and not a small effect ($A12 \ge 0.6$).
For a justification of the use of non-parametric bootstrapping, see Efron \& Tibshirani~\cite[p220-223]{efron93}. For a justification of the use of effect size tests see Shepperd\&MacDonell~\cite{shepperd12a}; Kampenes~\cite{kampenes07}; and Kocaguenli et al.~\cite{Kocaguneli2013:ep}. These researchers warn that even if a hypothesis test declares two populations to be ``significantly'' different, then that result is misleading if the ``effect size'' is very small. Hence, to assess the performance differences we first must rule out small effects using A12, a test   recently endorsed by Arcuri and Briand at ICSE'11~\cite{arcuri11}.

The Scott-Knott  results are presented in the form of line diagrams like those shown on the right-hand-side of \fig{jur}.
The black dot shows the median \eq{diff} values and the horizontal lines stretch 
from the 25th percentile to the 75th percentile (the inter-quartile range, IQR).
As an example of how to read this table, consider the {\em Ant}
results. Those rows are  sorted on the median values of each method. Note that all the methods have \eq{diff} \textgreater $ 0\%$; i.e. all these methods reduced the expected value of the performance score in that experiment while XTREE achieved the greatest reduction (of 56\% from the original value).
These results table has a  left-hand-side  {\bf Rank} column, computed using the
Scott-Knott test described above. In the {\em Ant}
results, XTREE is ranked the best, while CD is  ranked   worst.

\input{tex/rq2.tex}
\section{Results}
 
 
\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figs/changes01.png}
\caption{Results  from XTREE.
While \fig{counts} are the {\em number} of times a code metric was changed,
this  figure shows {\em how far} each code metric was changed. Each vertical bar
marks the 27,50,75th percentile change seen in 40 repeats.
All numbers are ratios of initial to final values.
All bar regions marked in gray show {\em increases}.
The interesting feature of these results are that many
of the changes proposed by XTREE require {\em increases}
(this puzzling observation is explained in the text).}
\label{fig:changes}
\end{figure*}



\subsection{RQ1: Effectiveness}

{\em Which of the methods defined in Section 3 is the best change oracle for identifying what and howcode modules should be changed? }

\fig{jur} shows the comparison results.  The Alves and Shatnawi
entries show the net effects of applying \eq{df} where the $t_i$ values were found by Alves and Shatnawi.
 In this part of the  analysis,
we applied Scott-Knott to all the \eq{diff} values
seen when code metrics were changed
according to the  Alves and Shatnawi


Two data sets are very responsive to defect reduction suggestions:
 Ant, and Ivy (both of which show best case improvements over 50\%).
 The  expected value of defects  
 is changed less in Jedit. This data sets' results
 surprisingly uniform; i.e.   all methods
 find the same ways to reduce the expected number of
defects.   For an explanation of the Jedit uniformity, see \tion{inc}.

Two data sets are not very responsive to defect reduction:
Poi and Lucene. The reason for this can be see in \fig{j}:
both these data sets contain more than 50\% defective modules.
In that space, all our  methods lack a large sample of
defect-free examples. 

Also consider the relative
rank of the different approaches,
 CD and Shatnawi usually  perform comparatively worse while  XTREE gets top ranked position the most
number of times. That said, Alves sometimes beats XTREE (see Ivy)
while sometimes it ties (see Jedit).

In summary, our {\bf Result1} is  that, of the change orcales studies here,
XTREE is the best oracle on how to change code modules (in order to reduce defects).

\input{tex/all_changes.tex}
 
 \subsection{RQ2: Verbosity}
 
 {\em Which of the Section 3 methods recommended changes to the fewest code attributes?}
 
\fig{counts} shows the frequency with which the methods
recommend changes to specific code methods.
Note that XTREE proposes thresholds to
few code metrics compared to the other approaches. 

% COMMENT From LL: This is such a strange way of putting this lesson, and you haven't actually evaluated this.
% 

Hence, our {\bf Result2} is that, of all the code change oracles studied here, XTREE recommended far fewest changes to static code measures.
Note only that,   combining  \fig{jur} with \fig{counts}, we   see that
even though XTREE proposes changes to far fewer code metrics, those few
code metrics are usually just as effective (or
more effective) than the multiple
thresholds
proposed by CD, Shatnawi or Alves.  That is, XTREE proposes
{\em fewer} and better thresholds than the other approaches studied here.
 


\subsection{RQ3: Stopping}

{\em  How effective is XTREE at offering   “stopping points” (i.e. clear guidance on what not to do)?}

The {\bf RQ2} results showed that XTREE's recommendations are small in a {\em relative sense}; i.e. they are 
relatively smaller than the other methods studied here.
Note only that, but XTREE's recommendations are small in an {\em absolute sense}.
Consider all frequency at which  \fig{counts} values for XTREE that are over 33\%; i.e. in at a third of our  repeated runs, XTREE mentioned
a code attribute. For Ant, Ivy, Lucene, Jedit, and Poi, those frequencies
are  \mbox{3, 3, 3, 4, 1, 2} respectively (out of twenty). This means that, usually, XTREE omits references to \mbox{17,17,17,16,19,18} static
code attributes (out of 20). 
Any refactoring based on a bad smell detector that uses  these omitted code attributes could hence  be stopped.

Hence our {\bf Result3} is that, in a any  project,  XTREE’s  recommended  changes  only  mentions one to four 
of the  static code attributes.  Any bad smell defined in terms of the remaining 19 to 16 code attributes (i.e. most of them)
would hence be deprecated.


\subsection{RQ4: Stability}

{\em Across different projects, how variable are the changes recommended by our best change oracle? }


\fig{counts} counted how {\em often} XTREE's recommendations mentioned a static code attribute.
\fig{changes}, on the other hand, shows the {\em direction} of the recommended change:
\bi
\item Gray bars show  an  {\em increase} to a static code measure;
\item White bars shows a   {\em drecrease} to a static code measure;
\item Bars that are all white or all gray indicate that in our 40 repeated runs, XTREE recommended changing an attribute the same way, all the time.
\item Bars that are mixtures of white and gray mean that, sometimes, XTREE makes different recommendations about how to change a static
code attribute.
\ei
Based on \fig{changes}, we say that {\bf Result4} is: the direction of change recommended  XTREE is  very stable repeated runs of the program  (evidence:
the bars are mostly the same color).


\subsection{RQ5: Stability}

{\em  Is  it  always  useful  to  apply \eq{df};  i.e.   make  code  better  by  reducing  the  values  of multiple code attributes?

Note that, in \fig{changes}, the bars are colored white and gray; i.e. XTREE recommends {\em decreasing} and {\em increasing} 
static attribute values. That is, always decreasing static code measures (as suggested by \eq{df}) is {\em not} recommended by XTREE.

To explore this point further, we conducted the following ``what-if'' study. Once XTREE builds its trees with its leaf clusters then:
\be
\item For all clusters $C_0$, $C_1$ where the percent of defects in $C_0$ is greater than $C_1$...
\item For all attribute measures that are have a statistically significantly different distribution  between $C_0$ and $C_1$...
\item Report the direction of the change.
\ee
Note that the changes found in this way are somewhat more general than those found by XTREE since XTREE's recommendations are limited
to comments on the test set given to the program. The above three points comments on the direction of {\em all possible changes}
that XTREE could ever report


Figure ~\ref{fig:multcomp} shows the results of this ``what if'' analysis. As in  \fig{changes}, there many increases
and decreases. Also, only with  cbo, rfc, loc,cam is a change recommended for a majority of the
data sets  (three of five, or more). 
XXXX

ups and down. fucntionalty has to go somehwere else


highlights all possible changes proposed by XTREE, upon close inspection it becomes apparent that XTREE suggests that it is perhaps better to reduce only certain forms of coupling. Refactoring operations that reduce one form of coupling at the cost of increasing another can sometimes be beneficial. This is most noticeable in Ivy, where one of the better approaches is to reduce coupling between objects (cbo) (efferent coupling) while increasing the afferent coupling (CA), again implying a redistribution of responsibilities. Using thresholds may not always appreciate the existence of these co-changes between metrics, and as a result many refactoring operations are usually left unexplored.

Note that we prefer XTREE to Shatnawi and the Alves
method since those other methods
do not reason
2about decreases {\em and} increases of multiple 
code metrics, at the same time.

As a final comment, we note
that  \fig{changes} enables us to explain the uniformity
of the results seen with Jedit in \fig{jur}.
Observe how in \fig{changes} the only change ever
found is a reduction to {\em rfc}. Clearly, in this
data set, there is very little that can be usefully changed.


\fig{multcomp} shows the frequency with which the methods
recommend changes to specific code methods.
Note that XTREE proposes thresholds to
few code metrics compared to the other approaches. 

When combining  \fig{jur}, \fig{counts}, and \fig{multcomp}, we   see that
even though XTREE proposes changes to far fewer code metrics, those few
code metrics are usually just as effective (or
more effective) than the multiple
thresholds
proposed by CD, Shatnawi or Alves.  That is, XTREE proposes
{\em fewer} and better thresholds than the other approaches studied here.

\begin{lesson}
In addition to recommending useful changes, XTREE recommends changes to all co-varying software metrics.
\end{lesson}


\section{Threats to Validity}\label{sect:valid}
XXXX assumes anything can change

assumes we can get data

assuems that defect is a;l;l.



Before  that, we offer one following note on a
core assumption of XTREE.
XTREE   is focused on Fowler's bad smells
as contributors to software defects. Some researchers 
 suggest that discussing bad smells is useful for more just reducing defects. For example, Bosu and Carver~\cite{bosu13} report that
debates about code smells and refactorings can (a)~introduce new comers to the code base or (b)~share knowledge on a team
about interactions within the code. In that view, 
the concept of refactoring can be useful
even if it does not address issues of defects. 


As with any empirical study, several issues can affect the final results. Therefore, any
conclusions made from this work must be offered with the following
caveats.

\subsection{External Validity}
Based on the experimental results above,
as well as the discussion in \tion{prelim},
we believe that bad smell indicators (e.g. \mbox{{\em loc}$>$ 100})
have limited external validity beyond the projects from which they are derived. 
While specific models are externally valid,
there may still be general methods like XTREE for finding the good local models.  


Our definition of bad smells is limited to those represented by OO code metrics (a premise often used in related work).   
XTREE, Shatnawi, Alves et al. can  only comment
on bad smells   expressed as code metrics 
  in the historical log of a project. 


If developers want to justify their refactorings
via bad smells expressed in other terminology,
then the  analysis of this paper must:
\begin{itemize}
    \item Either wait till 
data about those new
terms has been collected. 
\item Or, apply cutting edge transfer learning
methods~\cite{Nam15,Jing15} to map data from other projects
into the current one.
\end{itemize}
Note that the transfer learning approach would
be highly experimental and require more study
before it can be safely recommended.

Sampling bias threatens any data mining experiment; i.e., what matters
there may not be true here. For example, the data sets used here comes from Jureczko et al. and any biases in their selection procedures
threaten the validity of these results. 
That said,
the best we can do is define our methods and publicize our data and code so that other researchers can
try to repeat our results and, perhaps, point out a previously unknown bias
in our analysis. Hopefully, other researchers will emulate our methods in
order to repeat, refute, or improve our results. 

% Data mining is a large and active field and any single study can only use a small subset of the known classification algorithms.  
% That said, we have taken care to document in this paper the decisions made by engineering
% judgement that could effect our conclusions.







% \subsection{Trusting the Changes}\label{sect:trust}
%   XTREE is evaluated by  comparing
% predicted performance scores before and after a planner makes changes to the feature values of an example:
% After making those
% changes, we may have a new example that has never been seen before. Therefore, it must be asked
% {\em ``can we trust the predictions made on such new examples?''}
 
% To answer this question, we note
% that data miners explore two  ``clouds'' of data: (1) the cloud of training examples and (2) the  cloud   of test examples
% (for a visualization of these clouds, see \fig{howxy}).
% \begin{figure}[!t]
%   \includegraphics[width=\linewidth]{figs/2d.eps} 
%  % \includegraphics[width=1\linewidth]{figs/BDBC.eps}
% \caption{Gray, red, green show (1) training examples, (2) test examples and 
%   (3) tests that have been altered by planners.
%   This figure uses axes generated from the first two components of a PCA analysis of all points. 
% }\label{fig:howxy}
% \end{figure}
% We should mistrust the predictions made by a  model   if it is being applied to examples  that are
% too far away from the
% training cloud.
% To test for ``too far'', we can run a data mining experiment that tests how well
% a model learned from the training data applies to the test data. Such experiments return some performance value.

% Note that predictions  about changes that  fall within the space of the training+test data, will be at least
% as accurate as the predictions of the original test data. With this, we assert that the predictions for changes that move examples towards/away from the training data can be trusted more/less (respectively).

% Accordingly, we need {\em trust-increasing} planners to generate new examples {\em closer} to the
% training examples.  To see how this works, 
%  \fig{howxy} is from the {\em ivy} data
% set (one of the Jureczko data sets used in this paper). It shows: (1)~the training examples in gray, (2)~the test examples in red, and (3)~the
% changed  examples displaced after applying a plan (in green).
%  Note that the  the   changed examples
% cases  (shown in green)  fall closer to the training cases (shown in gray) than
% the test cases (shown in red). 

% In that green region of changed examples, our belief in the value of predictions
% will be just as much as, if not more than, our belief in the value of the predictions in the red region (that
% contains the original test data).
% This pattern of \fig{howxy} (where the new examples are found closer to  the training cases than the test cases) has been observed in all the other data sets studied in this
% paper. Hence,  we can assert that
% predictors learned from these training examples have some authority in the regions
% containing the changes examples.


% That said, the above comes with some important caveats:
% \bi
% \item 
% The quality of the prediction depends on the nature of the training data. Thus, we strongly recommend that both the data set and the predictor be assessed prior to planning. This ensures that the predictor's performance is adequate for a data set. We tackle this issue in detail in \tion{tesd}.
% \item
% Planners should be designed to be {\em trust increasing}. We list four such planning methods in \tion{planners}.
% \item
% Where possible, planners should be assessed via some external
% oracle that can accurately assess new examples. For an example of that kind of analysis,
% see  \tion{coc}.
% \ei

\subsection{  Internal Validity}\label{sect:coc}

% The main internal validity threat   in this study was the use of
% optimistic reasoning when generating the changed code modules.
We assumed in \tion{rq}
that any code module can be changed in order
to stop triggering a bad smell detector. This is not necessarily
true, as noted in {\em PROBLEM \#2} in this paper's introduction.
To mitigate this issue:
\begin{itemize}
    \item We do not propose using XTREE as a bad
smell generator;
\item Rather, we offer it as  a critique agent for bad smells
proposed by some other oracle (e.g. a human developer).
\end{itemize}
We showed about that XTREE's recommendations are not verbose.
That is, even if XTREE recommends the superset of possibly important bad smells,
that superset is not so large as to  overlap with many
 bad smell proposed at random 
by   developers. 

XTREE does not require a working defect predictor to generate
its bad smell thresholds. However, to {\em certify}
that those thresholds are satisfactory, some
second quality predictor must be applied. A data set containing defect counts before and after refactoring activities would enable an accurate assessment of the performance of the threshold-setting and recommender methods, however, such a data set is not available at this time.

 Historically,
SE researchers~\cite{Cheng10,OKeeffe08,OKeeffe07,Moghadam2011,Mkaouer14} have used the QMOOD hierarchical quality model~\cite{Bansiya02}.
Since such quality models may be very project specific~\cite{localvsglobal}, we prefer building local 
defect predictors using Random Forests. We chose this approach,  based on its reputation for having the better  performance of 21 other learners for defect prediction~\cite{lessmann}.  
Our results show that when a defect log contains too few
non-defective examples then defector predictors
perform poorly (see the data sets in the last few lines of \fig{j})
as do their associated bad smell threshold generators
(recall the performance of Lucene and Poi in~\fig{jur}). It should be noted that this is not a limitation of XTREE-- rather
it seems fundamental to the problem of certifying bad smells.
As evidence of this, recall that {\em all} the bad smell threshold generators of~\fig{jur}
had issues with the defect data sets with  many defects
(Lucene and Poi).


% \subsection{ Learner Bias}
% This study used two kinds of learners: (1) methods for suggesting
% changes to code like Shatnawi, Alves et al., CD and XTREE;
% (2) methods for assessing the changes proposed by these learners.
% Prior studies implemented (2) using  the QMOOD hierarchical quality model
% which is so old, that we suspect it may no longer be current. Hence,
% we used Random Forests to build the assessment oracles that were specific
% to the data sets used in this study. We chose this approach,  based on its reputation for having the better  performance of 21 other learners for defect prediction~\cite{lessmann}. 

  



% \subsection{  Sampling bias} 
% Sampling bias threatens any data mining experiment; i.e., what matters
% there may not be true here. For example, the data sets used here comes from two  Jureczko et al. and any biases in their selection procedures
% threaten the validity of these results. 
% That said,
% the best we can do is define our methods and publicize our data and code so that other researchers can
% try to repeat our results and, perhaps, point out a previously unknown bias
% in our analysis. Hopefully, other researchers will emulate our methods in
% order to repeat, refute, or improve our results. 





% \input{tex/results_models.tex}


\section{Conclusions}
Which  bad smells can  be ignored? 
We say: ignore those not supported by the historical log of data from
the current project.  

When that data is not available (e.g. early
in the project) then developers could use the general list of
bad smells shown in \fig{smells}. However, 
our results
show that  bad smell detectors are most
effective when they are based
on a small handful of code metrics (as done by XTREE).
Hence, using all the bad smells of \fig{smells} may not be optimal.

XTREE improves on prior methods for generating bad smells:
\begin{itemize}
    \item As described in \tion{prelim}, bad smells generated by humans may not be applicable to the current project. On the other hand, XTREE can automatically learn specific thresholds
    for bad smells for the current project.
    \item Prior methods used an old quality predictor (QMOOD) which we replace with defect predictors learned via Random Forests
    from   current project data.
    \item As described in in \tion{bst}, 
    XTREE generates conjunctions of code metric thresholds
    that need to be satisfied at the same time.
    Older methods, such as those proposed by  Shatnawi and Alves
    offer only disjunctions that might be inappropriately applied
    to yield impossible refactorings (e.g. reducing lines of code
    and coupling at the same time).
    
    \item XTREE's conjunctions proved to be arguably as effective
    as those of Alves (see \fig{jur}) but far less verbose (see \fig{counts}) thus making XTREE a better tool for critiquing and rejecting
    more bad smells proposed by developers.
\end{itemize}
In summary, this paper offers a baseline result
that XTREE can be used as a {\em diagnostic tool}
for identifying which bad smells can be ignored.
But can it be used as a {\em prescriptive tool}
to recommend new refactorings?  

As discussed in {\em PROBLEM \#2}
(from the introduction), XTREE might propose
changes to code metrics that are
not feasible  (due to the local domain constraints
of the code). To repair this, XTREE needs to be extended
with architectural knowledge of the code it is studying.
Elsewhere, we have had some   success
with reasoning about such architectural knowledge
(expressed as software product lines~\cite{sayyad13a,sayyad13b}). In future
work, we will explore extending XTREE 
with  
architectural knowledge such that it can
become a tool which perscribes
feasible refactorings. 


\section*{Acknowledgements}
The work is partially funded by NSF  awards \#1506586 and \#1302169.

% \bibliographystyle{plain}
\balance
\bibliographystyle{unsrt}
\bibliography{References}
\end{document}